{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYZiKtVWERv6u9rZwKNG2h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/colabcodes/blob/main/New_Question_Bank_26_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "sCNjA_U1NU2E",
        "outputId": "09bf916c-e3ec-4149-a5a0-7f547497cbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Enhanced Reading Comprehension Question Generator\n",
            "==================================================\n",
            "Enter your OpenAI API key: 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\n",
            "‚úÖ Using OpenAI v1.0+ API\n",
            "Enter output filename (without extension): sample3bank\n",
            "\n",
            "üìÅ Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "\n",
            "üìÑ Upload your CSV file with articles...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f821ca72-fb44-4f82-8ddd-1309104add87\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f821ca72-fb44-4f82-8ddd-1309104add87\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving article - Sheet1 (1).csv to article - Sheet1 (1).csv\n",
            "Processing: article - Sheet1 (1).csv\n",
            "Loaded 19 articles\n",
            "Processing 19 valid articles with source URLs...\n",
            "Enhanced version will create more natural, varied questions. This may take several hours for 1M articles.\n",
            "\n",
            "üîÑ Starting enhanced processing in 1 batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:26<00:00, 26.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/1: 133 questions generated in 26.9s (avg: 1.41s/article)\n",
            "\n",
            "‚úÖ Enhanced processing complete! Generated 133 questions from 19 articles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéâ Enhanced results saved successfully!\n",
            "üìÅ JSON file: /content/drive/MyDrive/sample3bank.json\n",
            "üìÅ CSV file: /content/drive/MyDrive/sample3bank.csv\n",
            "üìä Total questions generated: 133\n",
            "\n",
            "üìà Enhanced Summary Statistics:\n",
            "Domains: {'Technology': np.int64(91), 'Business': np.int64(14), 'Literature': np.int64(7), 'Sports': np.int64(7), 'Politics': np.int64(7), 'History': np.int64(7)}\n",
            "Stages: {'High School': np.int64(70), 'Middle School': np.int64(63)}\n",
            "Question Types: {'detail_comprehension': np.int64(38), 'inference_reasoning': np.int64(38), 'opinion_attitude': np.int64(38), 'main_idea': np.int64(19)}\n",
            "Answer Distribution: {'C': np.int64(51), 'B': np.int64(49), 'D': np.int64(18), 'A': np.int64(14), '': np.int64(1)}\n",
            "‚úÖ Clean answers (A-D only): 132/133 (99.2%)\n",
            "‚úÖ Properly formatted options: 133/133 (100.0%)\n",
            "\n",
            "üèÅ Enhanced process completed with improved quality!\n"
          ]
        }
      ],
      "source": [
        "# Reading Comprehension Question Generator for Google Colab\n",
        "# Enhanced version with natural question patterns and proper formatting\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import re\n",
        "import openai\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import files, drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# Configuration and Setup\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.api_key = None\n",
        "        self.model = \"gpt-3.5-turbo\"\n",
        "        self.max_workers = 5  # Optimize for speed while avoiding rate limits\n",
        "        self.batch_size = 100  # Process in batches to manage memory\n",
        "        self.max_retries = 3\n",
        "        self.retry_delay = 2\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Get API key and file settings\n",
        "print(\"üöÄ Enhanced Reading Comprehension Question Generator\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Input configurations\n",
        "config.api_key = input(\"Enter your OpenAI API key: \").strip()\n",
        "\n",
        "# Initialize OpenAI client (works with both old and new versions)\n",
        "try:\n",
        "    # Try new OpenAI v1.0+ client\n",
        "    from openai import OpenAI\n",
        "    openai_client = OpenAI(api_key=config.api_key)\n",
        "    use_new_api = True\n",
        "    print(\"‚úÖ Using OpenAI v1.0+ API\")\n",
        "except ImportError:\n",
        "    # Fallback to old API\n",
        "    openai.api_key = config.api_key\n",
        "    openai_client = openai\n",
        "    use_new_api = False\n",
        "    print(\"‚úÖ Using OpenAI legacy API\")\n",
        "\n",
        "output_filename = input(\"Enter output filename (without extension): \").strip()\n",
        "if not output_filename:\n",
        "    output_filename = \"reading_comprehension_output\"\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\nüìÅ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Enhanced question patterns with much more variety\n",
        "QUESTION_STARTERS = {\n",
        "    \"detail_comprehension\": [\n",
        "        \"Based on the passage, what\",\n",
        "        \"The text states that\",\n",
        "        \"According to the information provided\",\n",
        "        \"The passage mentions that\",\n",
        "        \"From the article, we learn that\",\n",
        "        \"The writer indicates that\",\n",
        "        \"The text reveals that\",\n",
        "        \"As described in the passage\",\n",
        "        \"The article explains that\",\n",
        "        \"The passage clearly shows that\",\n",
        "        \"We can determine from the text that\",\n",
        "        \"The information suggests that\",\n",
        "        \"The content indicates that\",\n",
        "        \"From what is written\",\n",
        "        \"The passage describes\",\n",
        "        \"The text identifies\",\n",
        "        \"According to the details given\",\n",
        "        \"The article specifies that\",\n",
        "        \"The writing confirms that\",\n",
        "        \"Based on the facts presented\"\n",
        "    ],\n",
        "    \"inference_reasoning\": [\n",
        "        \"It can be concluded that\",\n",
        "        \"The passage suggests that\",\n",
        "        \"We can infer that\",\n",
        "        \"The text implies that\",\n",
        "        \"One can reasonably assume that\",\n",
        "        \"The evidence points to\",\n",
        "        \"The information leads us to believe that\",\n",
        "        \"Based on the context\",\n",
        "        \"The underlying message is that\",\n",
        "        \"The passage hints that\",\n",
        "        \"We might deduce that\",\n",
        "        \"The text indicates that\",\n",
        "        \"From the evidence presented\",\n",
        "        \"The overall tone suggests that\",\n",
        "        \"Reading between the lines\",\n",
        "        \"The subtext reveals that\",\n",
        "        \"The implications are that\",\n",
        "        \"The logical conclusion is that\",\n",
        "        \"The passage conveys that\",\n",
        "        \"The deeper meaning suggests\"\n",
        "    ],\n",
        "    \"opinion_attitude\": [\n",
        "        \"The author's perspective on\",\n",
        "        \"The writer's viewpoint regarding\",\n",
        "        \"The author feels that\",\n",
        "        \"The writer's stance on\",\n",
        "        \"The author's opinion about\",\n",
        "        \"The text reflects a\",\n",
        "        \"The author demonstrates\",\n",
        "        \"The writer shows\",\n",
        "        \"The author's attitude toward\",\n",
        "        \"The writer expresses\",\n",
        "        \"The author believes that\",\n",
        "        \"The text conveys the author's\",\n",
        "        \"The writer's position is that\",\n",
        "        \"The author's approach to\",\n",
        "        \"The writer regards\",\n",
        "        \"The author considers\",\n",
        "        \"The text shows the author\",\n",
        "        \"The writer's judgment about\",\n",
        "        \"The author's take on\",\n",
        "        \"The writer's assessment of\"\n",
        "    ],\n",
        "    \"main_idea\": [\n",
        "        \"The central theme of this passage is\",\n",
        "        \"This text primarily focuses on\",\n",
        "        \"The main point of the passage is\",\n",
        "        \"The passage is essentially about\",\n",
        "        \"The primary subject of this text is\",\n",
        "        \"The core message of the passage is\",\n",
        "        \"This article mainly discusses\",\n",
        "        \"The fundamental topic is\",\n",
        "        \"The passage centers on\",\n",
        "        \"The key focus of the text is\",\n",
        "        \"The predominant theme is\",\n",
        "        \"The passage's main concern is\",\n",
        "        \"The central focus involves\",\n",
        "        \"The primary emphasis is on\",\n",
        "        \"The text is fundamentally about\",\n",
        "        \"The overarching theme is\",\n",
        "        \"The passage mainly addresses\",\n",
        "        \"The principal subject matter is\",\n",
        "        \"The text chiefly examines\",\n",
        "        \"The main thrust of the passage is\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Enhanced analysis starters that sound natural\n",
        "ANALYSIS_STARTERS = [\n",
        "    \"This is correct because\",\n",
        "    \"The passage supports this by\",\n",
        "    \"The text clearly indicates that\",\n",
        "    \"Based on the information provided\",\n",
        "    \"The author explicitly mentions that\",\n",
        "    \"The passage demonstrates that\",\n",
        "    \"The evidence shows that\",\n",
        "    \"The text confirms that\",\n",
        "    \"The article establishes that\",\n",
        "    \"The writing reveals that\",\n",
        "    \"The content supports this through\",\n",
        "    \"The passage provides evidence that\",\n",
        "    \"The information confirms that\",\n",
        "    \"The text validates this by\",\n",
        "    \"The author's words support this because\",\n",
        "    \"The passage substantiates this through\",\n",
        "    \"The evidence points to this since\",\n",
        "    \"The text backs this up by\",\n",
        "    \"The information corroborates this because\",\n",
        "    \"The passage verifies this through\"\n",
        "]\n",
        "\n",
        "# Domain classification function\n",
        "def classify_domain(text):\n",
        "    \"\"\"Classify article domain based on content keywords\"\"\"\n",
        "    domain_keywords = {\n",
        "        \"Technology\": [\"technology\", \"tech\", \"digital\", \"AI\", \"software\", \"internet\", \"computer\", \"smartphone\", \"app\"],\n",
        "        \"Science\": [\"research\", \"study\", \"scientist\", \"experiment\", \"discovery\", \"medical\", \"health\", \"biology\"],\n",
        "        \"Sports\": [\"sport\", \"athlete\", \"game\", \"competition\", \"team\", \"player\", \"championship\", \"olympic\"],\n",
        "        \"Business\": [\"business\", \"company\", \"market\", \"economy\", \"finance\", \"investment\", \"corporate\", \"industry\"],\n",
        "        \"Politics\": [\"government\", \"political\", \"policy\", \"election\", \"politician\", \"law\", \"congress\", \"president\"],\n",
        "        \"Entertainment\": [\"movie\", \"music\", \"celebrity\", \"film\", \"actor\", \"entertainment\", \"show\", \"television\"],\n",
        "        \"Education\": [\"school\", \"student\", \"education\", \"teacher\", \"university\", \"learning\", \"academic\", \"classroom\"],\n",
        "        \"Environment\": [\"environment\", \"climate\", \"nature\", \"pollution\", \"sustainable\", \"green\", \"ecology\", \"conservation\"],\n",
        "        \"History\": [\"history\", \"historical\", \"ancient\", \"past\", \"century\", \"era\", \"civilization\", \"culture\"],\n",
        "        \"Literature\": [\"book\", \"author\", \"novel\", \"poetry\", \"literature\", \"writing\", \"story\", \"literary\"]\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    domain_scores = {}\n",
        "\n",
        "    for domain, keywords in domain_keywords.items():\n",
        "        score = sum(text_lower.count(keyword) for keyword in keywords)\n",
        "        if score > 0:\n",
        "            domain_scores[domain] = score\n",
        "\n",
        "    return max(domain_scores, key=domain_scores.get) if domain_scores else \"General\"\n",
        "\n",
        "# Stage classification based on text complexity\n",
        "def classify_stage(text):\n",
        "    \"\"\"Classify educational stage based on text complexity\"\"\"\n",
        "    word_count = len(text.split())\n",
        "    avg_sentence_length = word_count / max(text.count('.') + text.count('!') + text.count('?'), 1)\n",
        "\n",
        "    # Simple heuristic based on length and complexity\n",
        "    if word_count < 200 or avg_sentence_length < 15:\n",
        "        return \"Middle School\"\n",
        "    else:\n",
        "        return \"High School\"\n",
        "\n",
        "# Clean and anonymize text\n",
        "def clean_and_anonymize_text(text):\n",
        "    \"\"\"Clean text and anonymize personal information\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Anonymize patterns\n",
        "    anonymize_patterns = [\n",
        "        (r'\\b\\d{3}-\\d{3}-\\d{4}\\b', 'xxx-xxx-xxxx'),  # Phone numbers\n",
        "        (r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', 'xxxx@xxxx.xxx'),  # Email\n",
        "        (r'\\b\\d{3,4}\\s?\\d{3,4}\\s?\\d{4}\\b', 'xxxxxxxxxxxx'),  # Various phone formats\n",
        "        (r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{4}\\b', 'xxxx-xxxx-xxxx-xxxx'),  # Credit card format\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement in anonymize_patterns:\n",
        "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Clean formatting\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text)  # Remove excessive line breaks\n",
        "    text = re.sub(r'\\s{3,}', ' ', text)  # Remove excessive spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Generate hash ID\n",
        "def generate_hash_id(text):\n",
        "    \"\"\"Generate unique hash ID for the article\"\"\"\n",
        "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Enhanced answer extraction and formatting\n",
        "def extract_clean_answer(answer_text):\n",
        "    \"\"\"Extract just the letter from answer field, removing explanations\"\"\"\n",
        "    if not answer_text:\n",
        "        return \"\"\n",
        "\n",
        "    # Clean the answer text\n",
        "    answer_text = str(answer_text).strip()\n",
        "\n",
        "    # Look for single letter answers (A, B, C, D) at the beginning\n",
        "    match = re.match(r'^([A-D])\\b', answer_text.upper())\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "\n",
        "    # If no clear match, try to find any A, B, C, or D\n",
        "    letters = re.findall(r'\\b([A-D])\\b', answer_text.upper())\n",
        "    if letters:\n",
        "        return letters[0]\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Enhanced options formatting\n",
        "def format_options_with_labels(options_list):\n",
        "    \"\"\"Format options with proper A, B, C, D labels\"\"\"\n",
        "    if not options_list or not isinstance(options_list, list):\n",
        "        return []\n",
        "\n",
        "    formatted_options = []\n",
        "    labels = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    for i, option in enumerate(options_list[:4]):  # Max 4 options\n",
        "        if i < len(labels):\n",
        "            # Clean the option text and add proper label\n",
        "            clean_option = str(option).strip()\n",
        "            # Remove any existing A), B), etc. labels\n",
        "            clean_option = re.sub(r'^[A-D][).]\\s*', '', clean_option)\n",
        "            formatted_options.append(f\"{labels[i]}. {clean_option}\")\n",
        "\n",
        "    return formatted_options\n",
        "\n",
        "# OpenAI API call with retry logic\n",
        "def call_openai_with_retry(messages, max_retries=3):\n",
        "    \"\"\"Call OpenAI API with retry logic for rate limiting\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if use_new_api:\n",
        "                # New OpenAI v1.0+ API\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    model=config.model,\n",
        "                    messages=messages,\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.8  # Increased for more variety\n",
        "                )\n",
        "                return response.choices[0].message.content.strip()\n",
        "            else:\n",
        "                # Legacy OpenAI API\n",
        "                response = openai_client.ChatCompletion.create(\n",
        "                    model=config.model,\n",
        "                    messages=messages,\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.8  # Increased for more variety\n",
        "                )\n",
        "                return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            error_str = str(e).lower()\n",
        "            if \"rate\" in error_str or \"limit\" in error_str or \"429\" in error_str:\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (2 ** attempt) * config.retry_delay\n",
        "                    print(f\"Rate limit hit, waiting {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise\n",
        "            elif attempt < max_retries - 1:\n",
        "                print(f\"Error occurred: {str(e)}, retrying...\")\n",
        "                time.sleep(config.retry_delay)\n",
        "            else:\n",
        "                raise\n",
        "    return None\n",
        "\n",
        "# Generate questions for a single article\n",
        "def generate_questions_for_article(article_data, article_index):\n",
        "    \"\"\"Generate 5-7 MC questions for a single article with natural variety\"\"\"\n",
        "    try:\n",
        "        article_text, source_url = article_data\n",
        "\n",
        "        # Clean and prepare text\n",
        "        clean_text = clean_and_anonymize_text(article_text)\n",
        "        if not clean_text or len(clean_text.split()) < 50:\n",
        "            return None\n",
        "\n",
        "        # Classify domain and stage\n",
        "        domain = classify_domain(clean_text)\n",
        "        stage = classify_stage(clean_text)\n",
        "\n",
        "        # Randomly select question starters for variety\n",
        "        detail_starters = random.sample(QUESTION_STARTERS[\"detail_comprehension\"], 2)\n",
        "        inference_starters = random.sample(QUESTION_STARTERS[\"inference_reasoning\"], 2)\n",
        "        opinion_starters = random.sample(QUESTION_STARTERS[\"opinion_attitude\"], 2)\n",
        "        main_idea_starter = random.choice(QUESTION_STARTERS[\"main_idea\"])\n",
        "\n",
        "        # Create enhanced prompt with natural variety\n",
        "        prompt = f\"\"\"\n",
        "You are a professional educator creating reading comprehension questions. Create EXACTLY 7 multiple choice questions based on this article. Make each question sound natural and unique - avoid repetitive patterns.\n",
        "\n",
        "ARTICLE:\n",
        "{clean_text}\n",
        "\n",
        "Create questions using these varied approaches:\n",
        "\n",
        "DETAIL COMPREHENSION (2 questions) - Use these starters:\n",
        "1. \"{detail_starters[0]}...\"\n",
        "2. \"{detail_starters[1]}...\"\n",
        "\n",
        "INFERENCE & REASONING (2 questions) - Use these starters:\n",
        "3. \"{inference_starters[0]}...\"\n",
        "4. \"{inference_starters[1]}...\"\n",
        "\n",
        "OPINION & ATTITUDE (2 questions) - Use these starters:\n",
        "5. \"{opinion_starters[0]}...\"\n",
        "6. \"{opinion_starters[1]}...\"\n",
        "\n",
        "MAIN IDEA (1 question) - Use this starter:\n",
        "7. \"{main_idea_starter}...\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Questions must sound natural and conversational\n",
        "- Base ALL questions on specific content from this article\n",
        "- Provide 4 options for each question (will be labeled A, B, C, D automatically)\n",
        "- Answer field should contain ONLY the letter (A, B, C, or D)\n",
        "- Make wrong answers plausible but clearly incorrect\n",
        "- Vary your language and avoid repetitive phrases\n",
        "- Analysis should explain why the answer is correct WITHOUT starting with \"The answer is X because\"\n",
        "\n",
        "Format as JSON:\n",
        "[\n",
        "  {{\n",
        "    \"type\": \"detail_comprehension\",\n",
        "    \"question\": \"Complete question using the starter provided...\",\n",
        "    \"options\": [\"First option\", \"Second option\", \"Third option\", \"Fourth option\"],\n",
        "    \"answer\": \"B\",\n",
        "    \"analysis\": \"This option is correct as the passage clearly states...\"\n",
        "  }}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert educator creating diverse, natural-sounding reading comprehension questions. Avoid repetitive patterns and make each question unique while maintaining educational value.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = call_openai_with_retry(messages)\n",
        "\n",
        "        if not response:\n",
        "            return None\n",
        "\n",
        "        # Parse JSON response\n",
        "        try:\n",
        "            questions_data = json.loads(response)\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback: try to extract JSON from response\n",
        "            json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    questions_data = json.loads(json_match.group())\n",
        "                except:\n",
        "                    return None\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # Process questions into required format\n",
        "        processed_questions = []\n",
        "        question_types = [\"detail_comprehension\", \"detail_comprehension\", \"inference_reasoning\",\n",
        "                         \"inference_reasoning\", \"opinion_attitude\", \"opinion_attitude\", \"main_idea\"]\n",
        "\n",
        "        for i, q_data in enumerate(questions_data[:7]):  # Max 7 questions\n",
        "            try:\n",
        "                hash_id = generate_hash_id(f\"{clean_text}_{i}\")\n",
        "                word_count = len(clean_text.split())\n",
        "\n",
        "                # Clean and format the answer\n",
        "                raw_answer = q_data.get('answer', '')\n",
        "                clean_answer = extract_clean_answer(raw_answer)\n",
        "\n",
        "                # Format options with A, B, C, D labels\n",
        "                raw_options = q_data.get('options', [])\n",
        "                formatted_options = format_options_with_labels(raw_options)\n",
        "\n",
        "                # Clean analysis (remove \"The answer is X because\" patterns)\n",
        "                raw_analysis = q_data.get('analysis', '')\n",
        "                clean_analysis = re.sub(r'^The answer is [A-D] because\\s*', '', raw_analysis)\n",
        "                clean_analysis = re.sub(r'^[A-D] is correct because\\s*', '', clean_analysis)\n",
        "\n",
        "                # Add natural analysis starter if needed\n",
        "                if not clean_analysis.strip():\n",
        "                    clean_analysis = \"The passage supports this option.\"\n",
        "                elif not clean_analysis[0].isupper():\n",
        "                    clean_analysis = clean_analysis[0].upper() + clean_analysis[1:]\n",
        "\n",
        "                question_obj = {\n",
        "                    \"id\": hash_id,\n",
        "                    \"text\": clean_text,  # Original article text\n",
        "                    \"meta\": {\n",
        "                        \"data_info\": {\n",
        "                            \"lang\": \"en\",\n",
        "                            \"source\": source_url,\n",
        "                            \"type\": \"Reading Comprehension\",\n",
        "                            \"processing_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "                            \"Question\": q_data.get('question', ''),\n",
        "                            \"Options\": formatted_options,  # Properly formatted with A, B, C, D\n",
        "                            \"Answer\": clean_answer,  # Just the letter\n",
        "                            \"Analysis\": clean_analysis,  # Natural analysis without repetitive starters\n",
        "                            \"Question_Type\": question_types[i] if i < len(question_types) else q_data.get('type', ''),\n",
        "                            \"word_count\": word_count\n",
        "                        },\n",
        "                        \"subject_info\": {\n",
        "                            \"subject\": \"English\",\n",
        "                            \"domain\": domain,\n",
        "                            \"stage\": stage\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "                processed_questions.append(question_obj)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing question {i} for article {article_index}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return processed_questions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {article_index}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Main processing function\n",
        "def process_articles_batch(articles_batch, start_index):\n",
        "    \"\"\"Process a batch of articles\"\"\"\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config.max_workers) as executor:\n",
        "        # Submit tasks\n",
        "        future_to_index = {\n",
        "            executor.submit(generate_questions_for_article, article_data, start_index + i): start_index + i\n",
        "            for i, article_data in enumerate(articles_batch) if article_data and article_data[0] and not pd.isna(article_data[0])\n",
        "        }\n",
        "\n",
        "        # Collect results\n",
        "        for future in as_completed(future_to_index):\n",
        "            index = future_to_index[future]\n",
        "            try:\n",
        "                questions = future.result(timeout=60)  # 60 second timeout\n",
        "                if questions:\n",
        "                    results.extend(questions)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing article {index}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load and process CSV file\n",
        "print(\"\\nüìÑ Upload your CSV file with articles...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "print(f\"Processing: {csv_filename}\")\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_filename)\n",
        "print(f\"Loaded {len(df)} articles\")\n",
        "\n",
        "# Detect columns (assume first column is articles, second is source URLs)\n",
        "article_column = df.columns[0]\n",
        "source_url_column = df.columns[1] if len(df.columns) > 1 else None\n",
        "\n",
        "# Create paired data (article, source_url)\n",
        "articles_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    if pd.notna(row[article_column]):\n",
        "        source_url = row[source_url_column] if source_url_column and pd.notna(row[source_url_column]) else \"Exam Question Bank\"\n",
        "        articles_data.append((row[article_column], source_url))\n",
        "\n",
        "print(f\"Processing {len(articles_data)} valid articles with source URLs...\")\n",
        "print(\"Enhanced version will create more natural, varied questions. This may take several hours for 1M articles.\")\n",
        "\n",
        "# Process articles in batches\n",
        "all_results = []\n",
        "batch_size = config.batch_size\n",
        "\n",
        "# Progress tracking\n",
        "total_batches = (len(articles_data) + batch_size - 1) // batch_size\n",
        "processed_articles = 0\n",
        "\n",
        "print(f\"\\nüîÑ Starting enhanced processing in {total_batches} batches...\")\n",
        "\n",
        "for batch_num in tqdm(range(0, len(articles_data), batch_size), desc=\"Processing batches\"):\n",
        "    batch = articles_data[batch_num:batch_num + batch_size]\n",
        "    start_time = time.time()\n",
        "\n",
        "    batch_results = process_articles_batch(batch, batch_num)\n",
        "    all_results.extend(batch_results)\n",
        "\n",
        "    processed_articles += len(batch)\n",
        "    batch_time = time.time() - start_time\n",
        "\n",
        "    # Memory management\n",
        "    if batch_num % 10 == 0:  # Every 10 batches\n",
        "        gc.collect()\n",
        "\n",
        "    # Progress info\n",
        "    questions_in_batch = len(batch_results)\n",
        "    avg_time_per_article = batch_time / len(batch) if batch else 0\n",
        "\n",
        "    print(f\"Batch {batch_num//batch_size + 1}/{total_batches}: {questions_in_batch} questions generated in {batch_time:.1f}s (avg: {avg_time_per_article:.2f}s/article)\")\n",
        "\n",
        "    # Save intermediate results every 1000 articles\n",
        "    if processed_articles % 1000 == 0:\n",
        "        temp_filename = f\"/content/drive/MyDrive/{output_filename}_temp_{processed_articles}.json\"\n",
        "        with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üíæ Saved intermediate results: {len(all_results)} questions\")\n",
        "\n",
        "print(f\"\\n‚úÖ Enhanced processing complete! Generated {len(all_results)} questions from {processed_articles} articles\")\n",
        "\n",
        "# Save final results\n",
        "if all_results:\n",
        "    # Save JSON\n",
        "    json_filename = f\"/content/drive/MyDrive/{output_filename}.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Create CSV version with enhanced structure\n",
        "    csv_data = []\n",
        "    for item in all_results:\n",
        "        # Format options as a clean string\n",
        "        options_str = \" | \".join(item['meta']['data_info']['Options']) if item['meta']['data_info']['Options'] else \"\"\n",
        "\n",
        "        flat_item = {\n",
        "            'id': item['id'],\n",
        "            'text': item['text'],  # Original article\n",
        "            'lang': item['meta']['data_info']['lang'],\n",
        "            'source': item['meta']['data_info']['source'],\n",
        "            'type': item['meta']['data_info']['type'],\n",
        "            'processing_date': item['meta']['data_info']['processing_date'],\n",
        "            'question': item['meta']['data_info']['Question'],\n",
        "            'options': options_str,  # Clean formatted options\n",
        "            'answer': item['meta']['data_info']['Answer'],  # Just the letter\n",
        "            'analysis': item['meta']['data_info']['Analysis'],  # Natural analysis\n",
        "            'question_type': item['meta']['data_info'].get('Question_Type', ''),\n",
        "            'word_count': item['meta']['data_info'].get('word_count', 0),\n",
        "            'subject': item['meta']['subject_info']['subject'],\n",
        "            'domain': item['meta']['subject_info']['domain'],\n",
        "            'stage': item['meta']['subject_info']['stage']\n",
        "        }\n",
        "        csv_data.append(flat_item)\n",
        "\n",
        "    csv_df = pd.DataFrame(csv_data)\n",
        "    csv_filename = f\"/content/drive/MyDrive/{output_filename}.csv\"\n",
        "    csv_df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "\n",
        "    print(f\"\\nüéâ Enhanced results saved successfully!\")\n",
        "    print(f\"üìÅ JSON file: {json_filename}\")\n",
        "    print(f\"üìÅ CSV file: {csv_filename}\")\n",
        "    print(f\"üìä Total questions generated: {len(all_results)}\")\n",
        "\n",
        "    # Enhanced summary statistics\n",
        "    domains = csv_df['domain'].value_counts()\n",
        "    stages = csv_df['stage'].value_counts()\n",
        "    question_types = csv_df['question_type'].value_counts()\n",
        "\n",
        "    # Check answer distribution\n",
        "    answer_dist = csv_df['answer'].value_counts()\n",
        "\n",
        "    print(f\"\\nüìà Enhanced Summary Statistics:\")\n",
        "    print(f\"Domains: {dict(domains)}\")\n",
        "    print(f\"Stages: {dict(stages)}\")\n",
        "    print(f\"Question Types: {dict(question_types)}\")\n",
        "    print(f\"Answer Distribution: {dict(answer_dist)}\")\n",
        "\n",
        "    # Quality checks\n",
        "    clean_answers = csv_df['answer'].str.match(r'^[A-D]$').sum()\n",
        "    print(f\"‚úÖ Clean answers (A-D only): {clean_answers}/{len(csv_df)} ({clean_answers/len(csv_df)*100:.1f}%)\")\n",
        "\n",
        "    proper_options = csv_df['options'].str.contains(r'A\\.|B\\.|C\\.|D\\.').sum()\n",
        "    print(f\"‚úÖ Properly formatted options: {proper_options}/{len(csv_df)} ({proper_options/len(csv_df)*100:.1f}%)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No questions were generated. Please check your input data and API key.\")\n",
        "\n",
        "print(\"\\nüèÅ Enhanced process completed with improved quality!\")"
      ]
    }
  ]
}