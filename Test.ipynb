{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9VgkizYLHNO7A7OhBXm9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/colabcodes/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpKB08eyGk3V"
      },
      "outputs": [],
      "source": [
        "# Adaptive AI Data Enrichment Agent v2.1 (Fixed Version)\n",
        "# Added better timeout handling and error recovery\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import Dict, List, Optional, Tuple, Any\n",
        "from dataclasses import dataclass\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import logging\n",
        "from openai import OpenAI\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration\n",
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the enrichment agent\"\"\"\n",
        "    perplexity_key: str = \"YOUR_PERPLEXITY_KEY\"\n",
        "    openai_key: str = \"YOUR_OPENAI_KEY\"\n",
        "    confidence_threshold: float = 0.7\n",
        "    max_retries: int = 2  # Reduced from 3\n",
        "    rate_limit_delay: float = 2.0  # Increased from 1.0\n",
        "    max_workers: int = 1  # Reduced from 3 for better control\n",
        "    request_timeout: int = 15  # Added timeout\n",
        "\n",
        "@dataclass\n",
        "class EnrichmentResult:\n",
        "    \"\"\"Data class for enrichment results\"\"\"\n",
        "    field: str\n",
        "    value: str\n",
        "    confidence: str\n",
        "    source: str\n",
        "    query_used: str\n",
        "    search_type: str\n",
        "\n",
        "class AdaptiveDataEnrichmentAgent:\n",
        "    \"\"\"\n",
        "    Data Enrichment Agent using Perplexity Search API\n",
        "    Enhanced with better timeout and error handling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.setup_logging()\n",
        "        self.setup_openai()\n",
        "        self.setup_perplexity()\n",
        "        self.enrichment_logs = []\n",
        "        self.failed_requests = 0\n",
        "        self.successful_requests = 0\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def setup_openai(self):\n",
        "        \"\"\"Setup OpenAI client with timeout\"\"\"\n",
        "        self.openai_client = OpenAI(\n",
        "            api_key=self.config.openai_key,\n",
        "            timeout=self.config.request_timeout\n",
        "        )\n",
        "\n",
        "    def setup_perplexity(self):\n",
        "        \"\"\"Setup Perplexity client using direct API calls with timeout\"\"\"\n",
        "        self.perplexity_url = \"https://api.perplexity.ai/chat/completions\"\n",
        "        self.perplexity_headers = {\n",
        "            \"Authorization\": f\"Bearer {self.config.perplexity_key}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"User-Agent\": \"DataEnrichmentAgent/2.1\"\n",
        "        }\n",
        "        self.use_native_client = False\n",
        "        self.default_model = \"sonar\"  # Updated to correct Perplexity model name\n",
        "        self.logger.info(\"Using direct API calls to Perplexity with timeout handling\")\n",
        "\n",
        "    def test_perplexity_connection(self) -> bool:\n",
        "        \"\"\"Test Perplexity API connection with timeout\"\"\"\n",
        "        try:\n",
        "            test_query = \"What is the capital of France?\"\n",
        "\n",
        "            payload = {\n",
        "                \"model\": self.default_model,\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": test_query}\n",
        "                ],\n",
        "                \"max_tokens\": 50,\n",
        "                \"temperature\": 0.1\n",
        "            }\n",
        "\n",
        "            self.logger.info(f\"Testing connection with model: {self.default_model}\")\n",
        "\n",
        "            response = requests.post(\n",
        "                self.perplexity_url,\n",
        "                json=payload,\n",
        "                headers=self.perplexity_headers,\n",
        "                timeout=self.config.request_timeout\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                content = data.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
        "                if content and ('Paris' in content or 'paris' in content.lower()):\n",
        "                    self.logger.info(\"Perplexity API connection test successful\")\n",
        "                    return True\n",
        "                else:\n",
        "                    self.logger.warning(f\"Unexpected test response: {content[:100]}...\")\n",
        "                    # Still consider it successful if we got a response\n",
        "                    return True\n",
        "            else:\n",
        "                self.logger.error(f\"Perplexity API test failed: {response.status_code}\")\n",
        "                self.logger.error(f\"Response: {response.text[:200]}...\")\n",
        "\n",
        "                # Try alternative models\n",
        "                alternative_models = [\n",
        "                    \"sonar-pro\",\n",
        "                    \"sonar-small-online\",\n",
        "                    \"sonar-medium-online\"\n",
        "                ]\n",
        "\n",
        "                for alt_model in alternative_models:\n",
        "                    self.logger.info(f\"Trying alternative model: {alt_model}\")\n",
        "                    payload[\"model\"] = alt_model\n",
        "\n",
        "                    try:\n",
        "                        alt_response = requests.post(\n",
        "                            self.perplexity_url,\n",
        "                            json=payload,\n",
        "                            headers=self.perplexity_headers,\n",
        "                            timeout=self.config.request_timeout\n",
        "                        )\n",
        "\n",
        "                        if alt_response.status_code == 200:\n",
        "                            self.logger.info(f\"Success with model: {alt_model}\")\n",
        "                            self.default_model = alt_model\n",
        "                            return True\n",
        "                    except Exception as e:\n",
        "                        self.logger.warning(f\"Model {alt_model} failed: {e}\")\n",
        "\n",
        "        except requests.exceptions.Timeout:\n",
        "            self.logger.error(\"Perplexity API test timed out\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Perplexity API test error: {e}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    def detect_identifiers_and_targets(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
        "        \"\"\"Detect identifier and target columns\"\"\"\n",
        "        columns = df.columns.tolist()\n",
        "        identifier_columns = []\n",
        "        target_columns = []\n",
        "\n",
        "        # Primary identifier patterns\n",
        "        strict_identifier_patterns = [\n",
        "            'company name', 'business name', 'organization name', 'name',\n",
        "            'website', 'url', 'domain', 'site'\n",
        "        ]\n",
        "\n",
        "        for col in columns:\n",
        "            col_lower = col.lower().replace('_', ' ').replace('-', ' ')\n",
        "\n",
        "            is_primary_identifier = False\n",
        "\n",
        "            for pattern in strict_identifier_patterns:\n",
        "                if pattern in col_lower:\n",
        "                    if pattern in ['company name', 'business name', 'organization name', 'name']:\n",
        "                        if col_lower in ['company name', 'business name', 'organization name', 'name'] or \\\n",
        "                           col_lower.endswith(' name'):\n",
        "                            is_primary_identifier = True\n",
        "                            break\n",
        "                    elif pattern in ['website', 'url', 'domain', 'site']:\n",
        "                        is_primary_identifier = True\n",
        "                        break\n",
        "\n",
        "            if is_primary_identifier:\n",
        "                identifier_columns.append(col)\n",
        "            else:\n",
        "                target_columns.append(col)\n",
        "\n",
        "        # Ensure we have at least one identifier\n",
        "        if not identifier_columns:\n",
        "            for col in columns:\n",
        "                col_lower = col.lower()\n",
        "                if 'name' in col_lower or 'company' in col_lower:\n",
        "                    identifier_columns.append(col)\n",
        "                    if col in target_columns:\n",
        "                        target_columns.remove(col)\n",
        "                    break\n",
        "\n",
        "        self.logger.info(f\"Identifiers: {identifier_columns}\")\n",
        "        self.logger.info(f\"Targets: {target_columns}\")\n",
        "\n",
        "        return identifier_columns, target_columns\n",
        "\n",
        "    def build_search_queries(self, target_field: str, company_name: str = None,\n",
        "                           website: str = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Build optimized search queries\"\"\"\n",
        "        queries = []\n",
        "\n",
        "        if company_name:\n",
        "            # Field-specific queries\n",
        "            field_lower = target_field.lower()\n",
        "\n",
        "            if 'industry' in field_lower:\n",
        "                queries.append({\n",
        "                    'query': f\"What industry is {company_name} in? What business sector does {company_name} operate in?\",\n",
        "                    'type': 'industry_specific',\n",
        "                    'priority': 1\n",
        "                })\n",
        "            elif 'employee' in field_lower and ('count' in field_lower or 'size' in field_lower):\n",
        "                queries.append({\n",
        "                    'query': f\"How many employees does {company_name} have? What is the employee count at {company_name}?\",\n",
        "                    'type': 'employee_specific',\n",
        "                    'priority': 1\n",
        "                })\n",
        "            elif 'headquarters' in field_lower or 'address' in field_lower:\n",
        "                queries.append({\n",
        "                    'query': f\"Where is {company_name} headquarters located? What is the address of {company_name}?\",\n",
        "                    'type': 'location_specific',\n",
        "                    'priority': 1\n",
        "                })\n",
        "            elif 'linkedin' in field_lower:\n",
        "                queries.append({\n",
        "                    'query': f\"What is the LinkedIn company page URL for {company_name}?\",\n",
        "                    'type': 'linkedin_specific',\n",
        "                    'priority': 1\n",
        "                })\n",
        "            else:\n",
        "                # General query\n",
        "                queries.append({\n",
        "                    'query': f\"Find information about {target_field} for company {company_name}\",\n",
        "                    'type': 'general',\n",
        "                    'priority': 2\n",
        "                })\n",
        "\n",
        "        return queries[:2]  # Limit to 2 queries\n",
        "\n",
        "    def call_perplexity(self, query: str) -> Optional[Dict]:\n",
        "        \"\"\"Make Perplexity API call with improved timeout handling\"\"\"\n",
        "        for attempt in range(self.config.max_retries):\n",
        "            try:\n",
        "                self.logger.info(f\"Perplexity Query (attempt {attempt + 1}): {query[:100]}...\")\n",
        "\n",
        "                payload = {\n",
        "                    \"model\": self.default_model,\n",
        "                    \"messages\": [\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": \"You are a business information researcher. Provide specific, factual information. Be concise and accurate.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": query\n",
        "                        }\n",
        "                    ],\n",
        "                    \"max_tokens\": 500,  # Reduced for faster responses\n",
        "                    \"temperature\": 0.1,\n",
        "                    \"top_p\": 0.9\n",
        "                }\n",
        "\n",
        "                response = requests.post(\n",
        "                    self.perplexity_url,\n",
        "                    json=payload,\n",
        "                    headers=self.perplexity_headers,\n",
        "                    timeout=self.config.request_timeout\n",
        "                )\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    self.successful_requests += 1\n",
        "                    self.logger.info(f\"Perplexity Success ({self.successful_requests} total)\")\n",
        "                    return data\n",
        "                elif response.status_code == 429:\n",
        "                    self.logger.warning(\"Rate limit hit, waiting longer...\")\n",
        "                    time.sleep(10)  # Wait 10 seconds for rate limit\n",
        "                    continue\n",
        "                else:\n",
        "                    self.logger.warning(f\"Perplexity HTTP {response.status_code}: {response.text[:200]}...\")\n",
        "                    self.failed_requests += 1\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                self.logger.error(f\"Request timed out after {self.config.request_timeout}s (attempt {attempt + 1})\")\n",
        "                self.failed_requests += 1\n",
        "            except requests.exceptions.ConnectionError:\n",
        "                self.logger.error(f\"Connection error (attempt {attempt + 1})\")\n",
        "                self.failed_requests += 1\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Perplexity Error (attempt {attempt + 1}): {e}\")\n",
        "                self.failed_requests += 1\n",
        "\n",
        "            # Progressive backoff\n",
        "            if attempt < self.config.max_retries - 1:\n",
        "                wait_time = self.config.rate_limit_delay * (2 ** attempt)\n",
        "                self.logger.info(f\"Waiting {wait_time}s before retry...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_from_perplexity_response(self, response: Dict, target_field: str) -> List[Dict]:\n",
        "        \"\"\"Extract data from Perplexity response\"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        try:\n",
        "            content = response['choices'][0]['message']['content']\n",
        "\n",
        "            # Use AI extraction\n",
        "            extracted_values = self._extract_values_with_ai(content, target_field)\n",
        "\n",
        "            for value_info in extracted_values:\n",
        "                candidates.append({\n",
        "                    'value': value_info['value'],\n",
        "                    'source': 'Perplexity Search',\n",
        "                    'confidence': value_info.get('confidence', 'medium'),\n",
        "                    'extraction_method': 'ai_extraction',\n",
        "                    'full_context': content[:300] + \"...\" if len(content) > 300 else content\n",
        "                })\n",
        "\n",
        "            self.logger.info(f\"Found {len(candidates)} candidates for {target_field}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting from response: {e}\")\n",
        "\n",
        "        return candidates\n",
        "\n",
        "    def _extract_values_with_ai(self, content: str, target_field: str) -> List[Dict]:\n",
        "        \"\"\"Extract values using OpenAI with timeout\"\"\"\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "            Extract the specific value for \"{target_field}\" from this text:\n",
        "\n",
        "            {content}\n",
        "\n",
        "            Return JSON format:\n",
        "            {{\n",
        "                \"values\": [\n",
        "                    {{\n",
        "                        \"value\": \"extracted clean value\",\n",
        "                        \"confidence\": \"high/medium/low\"\n",
        "                    }}\n",
        "                ]\n",
        "            }}\n",
        "\n",
        "            Guidelines:\n",
        "            - For industry: use standard industry terms\n",
        "            - For employee count: use numbers only like \"150\" or \"1,500\"\n",
        "            - For employee size: use ranges like \"50-200\" or descriptive terms\n",
        "            - For headquarters: use city, state/country format\n",
        "            - For LinkedIn: use full URL format\n",
        "            - Return empty array if no clear value found\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.openai_client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                response_format={\"type\": \"json_object\"},\n",
        "                temperature=0.1,\n",
        "                max_tokens=200,\n",
        "                timeout=10  # 10 second timeout\n",
        "            )\n",
        "\n",
        "            result = json.loads(response.choices[0].message.content)\n",
        "            return result.get(\"values\", [])\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"AI extraction failed: {e}\")\n",
        "            return self._simple_pattern_extraction(content, target_field)\n",
        "\n",
        "    def _simple_pattern_extraction(self, text: str, target_field: str) -> List[Dict]:\n",
        "        \"\"\"Simple pattern-based extraction as fallback\"\"\"\n",
        "        field_lower = target_field.lower()\n",
        "\n",
        "        # Basic patterns\n",
        "        if 'employee' in field_lower:\n",
        "            matches = re.findall(r'(\\d{1,6})\\s*(?:employees|staff|people)', text, re.IGNORECASE)\n",
        "            if matches:\n",
        "                return [{'value': matches[0], 'confidence': 'low'}]\n",
        "\n",
        "        elif 'industry' in field_lower:\n",
        "            # Look for common industry terms\n",
        "            industries = ['pharmaceutical', 'biotech', 'healthcare', 'technology', 'manufacturing',\n",
        "                         'software', 'medical', 'life sciences']\n",
        "            for industry in industries:\n",
        "                if industry in text.lower():\n",
        "                    return [{'value': industry.title(), 'confidence': 'low'}]\n",
        "\n",
        "        return []\n",
        "\n",
        "    def validate_and_normalize_with_ai(self, candidates: List[Dict],\n",
        "                                     target_field: str) -> Optional[EnrichmentResult]:\n",
        "        \"\"\"Validate candidates with AI\"\"\"\n",
        "        if not candidates:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Simple validation - take the first high confidence candidate\n",
        "            for candidate in candidates:\n",
        "                if candidate.get('confidence') == 'high' and candidate['value'].strip():\n",
        "                    return EnrichmentResult(\n",
        "                        field=target_field,\n",
        "                        value=candidate['value'].strip(),\n",
        "                        confidence=candidate['confidence'],\n",
        "                        source=candidate.get('source', 'Perplexity'),\n",
        "                        query_used=\"\",\n",
        "                        search_type=\"\"\n",
        "                    )\n",
        "\n",
        "            # Fallback to first candidate\n",
        "            if candidates and candidates[0]['value'].strip():\n",
        "                return EnrichmentResult(\n",
        "                    field=target_field,\n",
        "                    value=candidates[0]['value'].strip(),\n",
        "                    confidence='medium',\n",
        "                    source=candidates[0].get('source', 'Perplexity'),\n",
        "                    query_used=\"\",\n",
        "                    search_type=\"\"\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Validation failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def enrich_single_field(self, row_data: Dict, target_field: str,\n",
        "                          identifier_cols: List[str]) -> Optional[EnrichmentResult]:\n",
        "        \"\"\"Enrich a single field with timeout protection\"\"\"\n",
        "        # Skip if already has data\n",
        "        current_value = row_data.get(target_field)\n",
        "        if pd.notna(current_value) and str(current_value).strip():\n",
        "            return None\n",
        "\n",
        "        # Get identifiers\n",
        "        company_name = None\n",
        "        website = None\n",
        "\n",
        "        for col in identifier_cols:\n",
        "            col_lower = col.lower()\n",
        "            value = row_data.get(col)\n",
        "\n",
        "            if pd.notna(value) and str(value).strip():\n",
        "                if 'name' in col_lower or 'company' in col_lower:\n",
        "                    company_name = str(value).strip()\n",
        "                elif 'website' in col_lower or 'url' in col_lower:\n",
        "                    website = str(value).strip()\n",
        "\n",
        "        if not company_name and not website:\n",
        "            return None\n",
        "\n",
        "        self.logger.info(f\"Enriching '{target_field}' for: {company_name or website}\")\n",
        "\n",
        "        # Build queries\n",
        "        query_configs = self.build_search_queries(target_field, company_name, website)\n",
        "\n",
        "        for query_config in query_configs:\n",
        "            query = query_config['query']\n",
        "            search_type = query_config['type']\n",
        "\n",
        "            self.logger.info(f\"Trying {search_type}: {query[:50]}...\")\n",
        "\n",
        "            start_time = time.time()\n",
        "            response = self.call_perplexity(query)\n",
        "            elapsed_time = time.time() - start_time\n",
        "\n",
        "            if response:\n",
        "                candidates = self.extract_from_perplexity_response(response, target_field)\n",
        "\n",
        "                if candidates:\n",
        "                    result = self.validate_and_normalize_with_ai(candidates, target_field)\n",
        "                    if result:\n",
        "                        result.query_used = query\n",
        "                        result.search_type = search_type\n",
        "                        self.logger.info(f\"Success: '{result.value}' ({elapsed_time:.1f}s)\")\n",
        "                        return result\n",
        "\n",
        "            # Wait between attempts\n",
        "            time.sleep(self.config.rate_limit_delay)\n",
        "\n",
        "        self.logger.info(f\"No data found for {target_field}\")\n",
        "        return None\n",
        "\n",
        "    def enrich_dataframe(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[Dict]]:\n",
        "        \"\"\"Enrich DataFrame with progress tracking\"\"\"\n",
        "        self.logger.info(\"Starting DataFrame enrichment...\")\n",
        "\n",
        "        # Detect schema\n",
        "        identifier_cols, target_cols = self.detect_identifiers_and_targets(df)\n",
        "\n",
        "        if not identifier_cols:\n",
        "            raise ValueError(\"No identifier columns detected!\")\n",
        "\n",
        "        if not target_cols:\n",
        "            raise ValueError(\"No target columns detected!\")\n",
        "\n",
        "        # Create enriched copy\n",
        "        enriched_df = df.copy()\n",
        "        all_logs = []\n",
        "\n",
        "        total_fields = len(target_cols) * len(df)\n",
        "        enriched_count = 0\n",
        "        processed_count = 0\n",
        "\n",
        "        self.logger.info(f\"Processing {len(df)} rows × {len(target_cols)} fields = {total_fields} total\")\n",
        "\n",
        "        # Process each row\n",
        "        for row_idx, row in df.iterrows():\n",
        "            row_data = row.to_dict()\n",
        "            self.logger.info(f\"\\n--- Row {row_idx + 1}/{len(df)} ---\")\n",
        "\n",
        "            # Show identifiers\n",
        "            identifiers = {col: str(row_data.get(col, '')).strip()\n",
        "                          for col in identifier_cols\n",
        "                          if pd.notna(row_data.get(col)) and str(row_data.get(col)).strip()}\n",
        "\n",
        "            self.logger.info(f\"Identifiers: {identifiers}\")\n",
        "\n",
        "            # Process each target field\n",
        "            for target_field in target_cols:\n",
        "                processed_count += 1\n",
        "                try:\n",
        "                    start_time = time.time()\n",
        "\n",
        "                    result = self.enrich_single_field(row_data, target_field, identifier_cols)\n",
        "\n",
        "                    process_time = time.time() - start_time\n",
        "\n",
        "                    if result:\n",
        "                        enriched_df.at[row_idx, target_field] = result.value\n",
        "                        enriched_count += 1\n",
        "\n",
        "                        log_entry = {\n",
        "                            'timestamp': time.time(),\n",
        "                            'row_index': int(row_idx),\n",
        "                            'field': result.field,\n",
        "                            'original_value': row_data.get(target_field, ''),\n",
        "                            'enriched_value': result.value,\n",
        "                            'confidence': result.confidence,\n",
        "                            'source': result.source,\n",
        "                            'process_time_seconds': round(process_time, 2),\n",
        "                            'status': 'success'\n",
        "                        }\n",
        "                        all_logs.append(log_entry)\n",
        "\n",
        "                        self.logger.info(f\"✓ {target_field}: '{result.value}' [{result.confidence}]\")\n",
        "\n",
        "                    else:\n",
        "                        log_entry = {\n",
        "                            'timestamp': time.time(),\n",
        "                            'row_index': int(row_idx),\n",
        "                            'field': target_field,\n",
        "                            'original_value': row_data.get(target_field, ''),\n",
        "                            'enriched_value': None,\n",
        "                            'process_time_seconds': round(process_time, 2),\n",
        "                            'status': 'failed'\n",
        "                        }\n",
        "                        all_logs.append(log_entry)\n",
        "\n",
        "                        self.logger.info(f\"✗ {target_field}: No data found\")\n",
        "\n",
        "                    # Progress update\n",
        "                    progress = (processed_count / total_fields) * 100\n",
        "                    self.logger.info(f\"Progress: {processed_count}/{total_fields} ({progress:.1f}%)\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    self.logger.error(f\"Error with {target_field}: {e}\")\n",
        "\n",
        "                    log_entry = {\n",
        "                        'timestamp': time.time(),\n",
        "                        'row_index': int(row_idx),\n",
        "                        'field': target_field,\n",
        "                        'status': 'error',\n",
        "                        'error_message': str(e)\n",
        "                    }\n",
        "                    all_logs.append(log_entry)\n",
        "\n",
        "        # Summary\n",
        "        success_rate = (enriched_count / total_fields) * 100 if total_fields > 0 else 0\n",
        "\n",
        "        self.logger.info(f\"\\n=== ENRICHMENT COMPLETE ===\")\n",
        "        self.logger.info(f\"Processed: {total_fields} fields\")\n",
        "        self.logger.info(f\"Successful: {enriched_count} ({success_rate:.1f}%)\")\n",
        "        self.logger.info(f\"API Success Rate: {self.successful_requests}/{self.successful_requests + self.failed_requests}\")\n",
        "\n",
        "        return enriched_df, all_logs\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function with better error handling\"\"\"\n",
        "\n",
        "    print(\"Adaptive AI Data Enrichment Agent v2.1 (Fixed Version)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get API keys\n",
        "    print(\"\\nStep 1: Configuration\")\n",
        "\n",
        "    perplexity_key = input(\"Enter your Perplexity API key: \").strip()\n",
        "    openai_key = input(\"Enter your OpenAI API key: \").strip()\n",
        "\n",
        "    if not perplexity_key or not openai_key:\n",
        "        print(\"Both API keys are required!\")\n",
        "        return\n",
        "\n",
        "    config = Config(\n",
        "        perplexity_key=perplexity_key,\n",
        "        openai_key=openai_key,\n",
        "        request_timeout=15,\n",
        "        rate_limit_delay=2.0,\n",
        "        max_retries=2\n",
        "    )\n",
        "\n",
        "    # Initialize agent\n",
        "    print(\"\\nInitializing agent with improved timeout handling...\")\n",
        "    agent = AdaptiveDataEnrichmentAgent(config)\n",
        "\n",
        "    # Test connections\n",
        "    print(\"\\nTesting API connections...\")\n",
        "\n",
        "    if not agent.test_perplexity_connection():\n",
        "        print(\"ERROR: Perplexity API connection failed!\")\n",
        "        return\n",
        "    print(\"✓ Perplexity API working\")\n",
        "\n",
        "    try:\n",
        "        test_response = agent.openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Test\"}],\n",
        "            max_tokens=5,\n",
        "            timeout=10\n",
        "        )\n",
        "        print(\"✓ OpenAI API working\")\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: OpenAI API failed: {e}\")\n",
        "        return\n",
        "\n",
        "    # File upload\n",
        "    print(\"\\nStep 2: Upload CSV File\")\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        uploaded = files.upload()\n",
        "    except ImportError:\n",
        "        print(\"Not in Colab - enter filename manually\")\n",
        "        filename = input(\"Enter CSV filename: \").strip()\n",
        "        uploaded = {filename: None} if filename else {}\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No file provided!\")\n",
        "        return\n",
        "\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    print(f\"Processing: {filename}\")\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filename, encoding='utf-8')\n",
        "        print(f\"Loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "\n",
        "        # Show structure\n",
        "        identifier_cols, target_cols = agent.detect_identifiers_and_targets(df)\n",
        "\n",
        "        print(f\"\\nDetected Structure:\")\n",
        "        print(f\"  Identifiers: {identifier_cols}\")\n",
        "        print(f\"  Targets: {target_cols}\")\n",
        "\n",
        "        total_enrichments = len(df) * len(target_cols)\n",
        "        estimated_time = (total_enrichments * config.rate_limit_delay) / 60\n",
        "\n",
        "        print(f\"\\nEstimates:\")\n",
        "        print(f\"  Total enrichments: {total_enrichments}\")\n",
        "        print(f\"  Estimated time: {estimated_time:.1f} minutes\")\n",
        "\n",
        "        proceed = input(f\"\\nProceed? (y/n): \").strip().lower()\n",
        "        if proceed != 'y':\n",
        "            return\n",
        "\n",
        "        # Process\n",
        "        print(f\"\\nStarting enrichment with timeout protection...\")\n",
        "\n",
        "        start_time = time.time()\n",
        "        enriched_df, logs = agent.enrich_dataframe(df)\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        # Save results\n",
        "        output_csv = f\"enriched_{filename}\"\n",
        "        enriched_df.to_csv(output_csv, index=False)\n",
        "\n",
        "        # Show summary\n",
        "        successful = len([log for log in logs if log['status'] == 'success'])\n",
        "        failed = len([log for log in logs if log['status'] == 'failed'])\n",
        "        errors = len([log for log in logs if log['status'] == 'error'])\n",
        "\n",
        "        print(f\"\\n=== FINAL RESULTS ===\")\n",
        "        print(f\"Time: {total_time/60:.1f} minutes\")\n",
        "        print(f\"Successful: {successful}\")\n",
        "        print(f\"Failed: {failed}\")\n",
        "        print(f\"Errors: {errors}\")\n",
        "        print(f\"Success rate: {successful/(successful+failed+errors)*100:.1f}%\")\n",
        "        print(f\"Output saved: {output_csv}\")\n",
        "\n",
        "        # Download in Colab\n",
        "        try:\n",
        "            from google.colab import files as colab_files\n",
        "            colab_files.download(output_csv)\n",
        "        except ImportError:\n",
        "            pass\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}