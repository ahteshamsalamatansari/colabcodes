{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUKh7IV9Vg5vjWJvgA9IF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/colabcodes/blob/main/MIT_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odB1R6ay1Nyv"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "MIT News Articles Scraper\n",
        "Optimized for scraping 10,000+ articles in ~40 minutes\n",
        "Run this entire cell in Google Colab with a single click\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import logging\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging for transparency\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MITNewsScraper:\n",
        "    def __init__(self, max_workers=20, request_timeout=10, retry_attempts=3):\n",
        "        \"\"\"\n",
        "        Initialize the scraper with optimized settings for speed\n",
        "\n",
        "        Args:\n",
        "            max_workers (int): Number of concurrent threads (20 for 40min target)\n",
        "            request_timeout (int): Request timeout in seconds\n",
        "            retry_attempts (int): Number of retry attempts for failed requests\n",
        "        \"\"\"\n",
        "        self.max_workers = max_workers\n",
        "        self.request_timeout = request_timeout\n",
        "        self.retry_attempts = retry_attempts\n",
        "        self.session = requests.Session()\n",
        "\n",
        "        # Optimize session for speed\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        })\n",
        "\n",
        "        # Results storage\n",
        "        self.successful_scrapes = []\n",
        "        self.failed_scrapes = []\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean extracted text from unwanted characters and formatting\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace and newlines\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Remove common unwanted patterns\n",
        "        text = re.sub(r'\\[.*?\\]', '', text)  # Remove square brackets content\n",
        "        text = re.sub(r'\\(.*?email.*?\\)', '', text, flags=re.IGNORECASE)  # Remove email references\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_article_content(self, soup):\n",
        "        \"\"\"\n",
        "        Extract clean article content from BeautifulSoup object\n",
        "\n",
        "        Args:\n",
        "            soup: BeautifulSoup parsed HTML\n",
        "\n",
        "        Returns:\n",
        "            tuple: (title, content)\n",
        "        \"\"\"\n",
        "        title = \"\"\n",
        "        content = \"\"\n",
        "\n",
        "        try:\n",
        "            # Extract title - multiple possible selectors\n",
        "            title_selectors = [\n",
        "                'h1.news-article--headline',\n",
        "                'h1[itemprop=\"headline\"]',\n",
        "                'h1.article-title',\n",
        "                'h1',\n",
        "                '.news-article--headline',\n",
        "                '[itemprop=\"headline\"]'\n",
        "            ]\n",
        "\n",
        "            for selector in title_selectors:\n",
        "                title_elem = soup.select_one(selector)\n",
        "                if title_elem:\n",
        "                    title = self.clean_text(title_elem.get_text())\n",
        "                    break\n",
        "\n",
        "            # Extract main content - target the specific content area\n",
        "            content_selectors = [\n",
        "                '.news-article--content--body--inner',\n",
        "                '.news-article--content--body',\n",
        "                '[itemprop=\"articleBody\"]',\n",
        "                '.article-body',\n",
        "                '.content-body'\n",
        "            ]\n",
        "\n",
        "            content_container = None\n",
        "            for selector in content_selectors:\n",
        "                content_container = soup.select_one(selector)\n",
        "                if content_container:\n",
        "                    break\n",
        "\n",
        "            if content_container:\n",
        "                # Extract only paragraph text, avoiding unwanted elements\n",
        "                paragraphs = []\n",
        "\n",
        "                # Find all paragraph tags\n",
        "                for p_tag in content_container.find_all('p'):\n",
        "                    # Skip paragraphs that are likely social media or navigation\n",
        "                    p_text = p_tag.get_text().strip()\n",
        "\n",
        "                    # Skip if paragraph contains social media indicators\n",
        "                    social_indicators = ['share', 'tweet', 'facebook', 'linkedin', 'reddit', 'print', 'follow us']\n",
        "                    if any(indicator in p_text.lower() for indicator in social_indicators):\n",
        "                        continue\n",
        "\n",
        "                    # Skip very short paragraphs (likely navigation)\n",
        "                    if len(p_text) < 10:\n",
        "                        continue\n",
        "\n",
        "                    # Skip paragraphs with mostly links\n",
        "                    links_count = len(p_tag.find_all('a'))\n",
        "                    words_count = len(p_text.split())\n",
        "                    if links_count > 0 and words_count < 20 and links_count >= words_count / 5:\n",
        "                        continue\n",
        "\n",
        "                    if p_text:\n",
        "                        paragraphs.append(self.clean_text(p_text))\n",
        "\n",
        "                content = ' '.join(paragraphs)\n",
        "\n",
        "            # Fallback if no content found\n",
        "            if not content:\n",
        "                # Try to get all text from body, then clean heavily\n",
        "                body = soup.find('body')\n",
        "                if body:\n",
        "                    # Remove script and style elements\n",
        "                    for script in body([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
        "                        script.decompose()\n",
        "                    content = self.clean_text(body.get_text())\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting content: {str(e)}\")\n",
        "\n",
        "        return title, content\n",
        "\n",
        "    def scrape_single_article(self, url):\n",
        "        \"\"\"\n",
        "        Scrape a single article with retry logic\n",
        "\n",
        "        Args:\n",
        "            url (str): Article URL to scrape\n",
        "\n",
        "        Returns:\n",
        "            dict: Scraped article data or None if failed\n",
        "        \"\"\"\n",
        "        for attempt in range(self.retry_attempts):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=self.request_timeout)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Parse HTML\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # Extract content\n",
        "                title, content = self.extract_article_content(soup)\n",
        "\n",
        "                if title and content and len(content) > 100:  # Minimum content length\n",
        "                    return {\n",
        "                        'url': url,\n",
        "                        'title': title,\n",
        "                        'content': content,\n",
        "                        'status': 'success',\n",
        "                        'content_length': len(content),\n",
        "                        'attempt': attempt + 1\n",
        "                    }\n",
        "                else:\n",
        "                    logger.warning(f\"Insufficient content for {url}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < self.retry_attempts - 1:\n",
        "                    time.sleep(1 * (attempt + 1))  # Exponential backoff\n",
        "                    continue\n",
        "                logger.error(f\"Failed to scrape {url} after {self.retry_attempts} attempts: {str(e)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error scraping {url}: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': '',\n",
        "            'content': '',\n",
        "            'status': 'failed',\n",
        "            'content_length': 0,\n",
        "            'attempt': self.retry_attempts\n",
        "        }\n",
        "\n",
        "    def scrape_articles(self, urls):\n",
        "        \"\"\"\n",
        "        Scrape multiple articles concurrently with progress visualization\n",
        "\n",
        "        Args:\n",
        "            urls (list): List of URLs to scrape\n",
        "\n",
        "        Returns:\n",
        "            tuple: (successful_results, failed_results)\n",
        "        \"\"\"\n",
        "        print(f\"ðŸš€ Starting to scrape {len(urls)} articles...\")\n",
        "        print(f\"âš™ï¸  Configuration: {self.max_workers} threads, {self.request_timeout}s timeout\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress tracking\n",
        "        with tqdm(total=len(urls), desc=\"Scraping Progress\", ncols=100) as pbar:\n",
        "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                # Submit all tasks\n",
        "                future_to_url = {executor.submit(self.scrape_single_article, url): url for url in urls}\n",
        "\n",
        "                # Process completed tasks\n",
        "                for future in as_completed(future_to_url):\n",
        "                    result = future.result()\n",
        "\n",
        "                    if result['status'] == 'success':\n",
        "                        self.successful_scrapes.append(result)\n",
        "                        pbar.set_postfix({\n",
        "                            'Success': len(self.successful_scrapes),\n",
        "                            'Failed': len(self.failed_scrapes),\n",
        "                            'Rate': f\"{len(self.successful_scrapes)/(time.time()-start_time):.1f}/s\"\n",
        "                        })\n",
        "                    else:\n",
        "                        self.failed_scrapes.append(result)\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Print final statistics\n",
        "        print(f\"\\nðŸ“Š Scraping Results:\")\n",
        "        print(f\"âœ… Successful: {len(self.successful_scrapes)}\")\n",
        "        print(f\"âŒ Failed: {len(self.failed_scrapes)}\")\n",
        "        print(f\"â±ï¸  Total time: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
        "        print(f\"ðŸ“ˆ Average rate: {len(urls)/duration:.2f} articles/second\")\n",
        "        print(f\"ðŸ’¾ Total content scraped: {sum(r['content_length'] for r in self.successful_scrapes):,} characters\")\n",
        "\n",
        "        return self.successful_scrapes, self.failed_scrapes\n",
        "\n",
        "def load_urls_from_text(url_text):\n",
        "    \"\"\"Load URLs from text input, handling various formats\"\"\"\n",
        "    urls = []\n",
        "    for line in url_text.strip().split('\\n'):\n",
        "        line = line.strip()\n",
        "        if line and line.startswith('http'):\n",
        "            urls.append(line)\n",
        "    return urls\n",
        "\n",
        "# MAIN EXECUTION SECTION - MODIFY THIS PART\n",
        "def main():\n",
        "    \"\"\"Main function - modify URLs here or upload file\"\"\"\n",
        "\n",
        "    print(\"ðŸ”§ MIT News Articles Scraper\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # OPTION 1: Paste URLs directly here (for testing)\n",
        "    sample_urls = [\n",
        "        \"https://news.mit.edu/2025/ianacare-builds-lifeline-for-family-caregivers-across-us-0811\",\n",
        "        \"https://news.mit.edu/2025/cloudian-helps-data-storage-keep-up-with-ai-revolution-0806\",\n",
        "        \"https://news.mit.edu/2025/ushering-new-era-suture-free-tissue-reconstruction-better-healing-0801\",\n",
        "        \"https://news.mit.edu/2025/supporting-mission-driven-space-innovation-aurelia-institute-0710\",\n",
        "        \"https://news.mit.edu/2025/new-platform-foundation-alloy-developing-advanced-metals-scale-0703\"\n",
        "    ]\n",
        "\n",
        "    # OPTION 2: Upload file method (uncomment and modify path)\n",
        "    # try:\n",
        "    #     with open('your_urls_file.txt', 'r') as f:\n",
        "    #         urls_text = f.read()\n",
        "    #         urls = load_urls_from_text(urls_text)\n",
        "    # except FileNotFoundError:\n",
        "    #     print(\"URLs file not found, using sample URLs\")\n",
        "    #     urls = sample_urls\n",
        "\n",
        "    # For now, using sample URLs\n",
        "    urls = sample_urls\n",
        "\n",
        "    if not urls:\n",
        "        print(\"âŒ No URLs found! Please add URLs to the sample_urls list or upload a file.\")\n",
        "        return\n",
        "\n",
        "    print(f\"ðŸ“‹ Found {len(urls)} URLs to scrape\")\n",
        "\n",
        "    # Initialize scraper with optimized settings for speed\n",
        "    scraper = MITNewsScraper(\n",
        "        max_workers=20,  # Adjust based on your needs (higher = faster but more aggressive)\n",
        "        request_timeout=10,\n",
        "        retry_attempts=2\n",
        "    )\n",
        "\n",
        "    # Start scraping\n",
        "    successful_results, failed_results = scraper.scrape_articles(urls)\n",
        "\n",
        "    # Save results to CSV\n",
        "    if successful_results:\n",
        "        df = pd.DataFrame(successful_results)\n",
        "        df = df[['url', 'title', 'content', 'content_length']]  # Select relevant columns\n",
        "\n",
        "        # Save to CSV\n",
        "        output_filename = f'mit_news_articles_{int(time.time())}.csv'\n",
        "        df.to_csv(output_filename, index=False)\n",
        "        print(f\"ðŸ’¾ Results saved to: {output_filename}\")\n",
        "\n",
        "        # Display sample results\n",
        "        print(f\"\\nðŸ“– Sample of scraped content:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, result in enumerate(successful_results[:2]):  # Show first 2 results\n",
        "            print(f\"Title: {result['title']}\")\n",
        "            print(f\"Content preview: {result['content'][:200]}...\")\n",
        "            print(f\"Content length: {result['content_length']} characters\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Save failed URLs for retry if needed\n",
        "    if failed_results:\n",
        "        failed_df = pd.DataFrame(failed_results)\n",
        "        failed_filename = f'failed_urls_{int(time.time())}.csv'\n",
        "        failed_df.to_csv(failed_filename, index=False)\n",
        "        print(f\"âš ï¸  Failed URLs saved to: {failed_filename}\")\n",
        "\n",
        "# Execute the scraper\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# FOR GOOGLE COLAB: Uncomment the line below to run automatically\n",
        "# main()"
      ]
    }
  ]
}