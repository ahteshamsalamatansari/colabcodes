{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUKh7IV9Vg5vjWJvgA9IF5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab08586a6efd493aace65cb58cc1c447": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdee57e22d5546cd98f83fd7758f2290",
              "IPY_MODEL_5d08adaa5875423480a9fd85f0c72e95",
              "IPY_MODEL_766fe30fc0f84caf9790552737b5fd96"
            ],
            "layout": "IPY_MODEL_c3d11b0e4ddf4c81b3a831e4d950b5e6"
          }
        },
        "fdee57e22d5546cd98f83fd7758f2290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e672d638f3a84c81a76ff3374cc2933d",
            "placeholder": "​",
            "style": "IPY_MODEL_328aca83303e47d8afa0179127125bbb",
            "value": "Scraping Progress: 100%"
          }
        },
        "5d08adaa5875423480a9fd85f0c72e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_352bb25758c942bd92beba6f74564812",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2988967149ab4476b481d1ba4161f0a2",
            "value": 5
          }
        },
        "766fe30fc0f84caf9790552737b5fd96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3efc5e2227843d982d954b4f799fd0c",
            "placeholder": "​",
            "style": "IPY_MODEL_d06545a0134e41c69117075a556f2a46",
            "value": " 5/5 [00:00&lt;00:00,  9.29it/s, Success=5, Failed=0, Rate=11.6/s]"
          }
        },
        "c3d11b0e4ddf4c81b3a831e4d950b5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100px"
          }
        },
        "e672d638f3a84c81a76ff3374cc2933d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328aca83303e47d8afa0179127125bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "352bb25758c942bd92beba6f74564812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2988967149ab4476b481d1ba4161f0a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3efc5e2227843d982d954b4f799fd0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d06545a0134e41c69117075a556f2a46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/colabcodes/blob/main/MIT_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562,
          "referenced_widgets": [
            "ab08586a6efd493aace65cb58cc1c447",
            "fdee57e22d5546cd98f83fd7758f2290",
            "5d08adaa5875423480a9fd85f0c72e95",
            "766fe30fc0f84caf9790552737b5fd96",
            "c3d11b0e4ddf4c81b3a831e4d950b5e6",
            "e672d638f3a84c81a76ff3374cc2933d",
            "328aca83303e47d8afa0179127125bbb",
            "352bb25758c942bd92beba6f74564812",
            "2988967149ab4476b481d1ba4161f0a2",
            "f3efc5e2227843d982d954b4f799fd0c",
            "d06545a0134e41c69117075a556f2a46"
          ]
        },
        "id": "odB1R6ay1Nyv",
        "outputId": "c787d87f-d15f-4db8-a3cb-b71f549e6252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 MIT News Articles Scraper\n",
            "==================================================\n",
            "📋 Found 5 URLs to scrape\n",
            "🚀 Starting to scrape 5 articles...\n",
            "⚙️  Configuration: 20 threads, 10s timeout\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Scraping Progress:   0%|                                                      | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab08586a6efd493aace65cb58cc1c447"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Scraping Results:\n",
            "✅ Successful: 5\n",
            "❌ Failed: 0\n",
            "⏱️  Total time: 0.44 seconds (0.01 minutes)\n",
            "📈 Average rate: 11.41 articles/second\n",
            "💾 Total content scraped: 33,586 characters\n",
            "💾 Results saved to: mit_news_articles_1755682593.csv\n",
            "\n",
            "📖 Sample of scraped content:\n",
            "--------------------------------------------------\n",
            "Title: Building a lifeline for family caregivers across the US\n",
            "Content preview: There are 63 million people caring for family members with an illness or disability in the U.S. That translates to one in four adults devoting their time to helping loved ones with things like transpo...\n",
            "Content length: 6068 characters\n",
            "--------------------------------------------------\n",
            "Title: Helping data storage keep up with the AI revolution\n",
            "Content preview: Artificial intelligence is changing the way businesses store and access their data. That’s because traditional data storage systems were designed to handle simple commands from a handful of users at o...\n",
            "Content length: 7526 characters\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "MIT News Articles Scraper\n",
        "Optimized for scraping 10,000+ articles in ~40 minutes\n",
        "Run this entire cell in Google Colab with a single click\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import logging\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging for transparency\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MITNewsScraper:\n",
        "    def __init__(self, max_workers=20, request_timeout=10, retry_attempts=3):\n",
        "        \"\"\"\n",
        "        Initialize the scraper with optimized settings for speed\n",
        "\n",
        "        Args:\n",
        "            max_workers (int): Number of concurrent threads (20 for 40min target)\n",
        "            request_timeout (int): Request timeout in seconds\n",
        "            retry_attempts (int): Number of retry attempts for failed requests\n",
        "        \"\"\"\n",
        "        self.max_workers = max_workers\n",
        "        self.request_timeout = request_timeout\n",
        "        self.retry_attempts = retry_attempts\n",
        "        self.session = requests.Session()\n",
        "\n",
        "        # Optimize session for speed\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "        })\n",
        "\n",
        "        # Results storage\n",
        "        self.successful_scrapes = []\n",
        "        self.failed_scrapes = []\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean extracted text from unwanted characters and formatting\"\"\"\n",
        "        if not text:\n",
        "            return \"\"\n",
        "\n",
        "        # Remove extra whitespace and newlines\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "        # Remove common unwanted patterns\n",
        "        text = re.sub(r'\\[.*?\\]', '', text)  # Remove square brackets content\n",
        "        text = re.sub(r'\\(.*?email.*?\\)', '', text, flags=re.IGNORECASE)  # Remove email references\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def extract_article_content(self, soup):\n",
        "        \"\"\"\n",
        "        Extract clean article content from BeautifulSoup object\n",
        "\n",
        "        Args:\n",
        "            soup: BeautifulSoup parsed HTML\n",
        "\n",
        "        Returns:\n",
        "            tuple: (title, content)\n",
        "        \"\"\"\n",
        "        title = \"\"\n",
        "        content = \"\"\n",
        "\n",
        "        try:\n",
        "            # Extract title - multiple possible selectors\n",
        "            title_selectors = [\n",
        "                'h1.news-article--headline',\n",
        "                'h1[itemprop=\"headline\"]',\n",
        "                'h1.article-title',\n",
        "                'h1',\n",
        "                '.news-article--headline',\n",
        "                '[itemprop=\"headline\"]'\n",
        "            ]\n",
        "\n",
        "            for selector in title_selectors:\n",
        "                title_elem = soup.select_one(selector)\n",
        "                if title_elem:\n",
        "                    title = self.clean_text(title_elem.get_text())\n",
        "                    break\n",
        "\n",
        "            # Extract main content - target the specific content area\n",
        "            content_selectors = [\n",
        "                '.news-article--content--body--inner',\n",
        "                '.news-article--content--body',\n",
        "                '[itemprop=\"articleBody\"]',\n",
        "                '.article-body',\n",
        "                '.content-body'\n",
        "            ]\n",
        "\n",
        "            content_container = None\n",
        "            for selector in content_selectors:\n",
        "                content_container = soup.select_one(selector)\n",
        "                if content_container:\n",
        "                    break\n",
        "\n",
        "            if content_container:\n",
        "                # Extract only paragraph text, avoiding unwanted elements\n",
        "                paragraphs = []\n",
        "\n",
        "                # Find all paragraph tags\n",
        "                for p_tag in content_container.find_all('p'):\n",
        "                    # Skip paragraphs that are likely social media or navigation\n",
        "                    p_text = p_tag.get_text().strip()\n",
        "\n",
        "                    # Skip if paragraph contains social media indicators\n",
        "                    social_indicators = ['share', 'tweet', 'facebook', 'linkedin', 'reddit', 'print', 'follow us']\n",
        "                    if any(indicator in p_text.lower() for indicator in social_indicators):\n",
        "                        continue\n",
        "\n",
        "                    # Skip very short paragraphs (likely navigation)\n",
        "                    if len(p_text) < 10:\n",
        "                        continue\n",
        "\n",
        "                    # Skip paragraphs with mostly links\n",
        "                    links_count = len(p_tag.find_all('a'))\n",
        "                    words_count = len(p_text.split())\n",
        "                    if links_count > 0 and words_count < 20 and links_count >= words_count / 5:\n",
        "                        continue\n",
        "\n",
        "                    if p_text:\n",
        "                        paragraphs.append(self.clean_text(p_text))\n",
        "\n",
        "                content = ' '.join(paragraphs)\n",
        "\n",
        "            # Fallback if no content found\n",
        "            if not content:\n",
        "                # Try to get all text from body, then clean heavily\n",
        "                body = soup.find('body')\n",
        "                if body:\n",
        "                    # Remove script and style elements\n",
        "                    for script in body([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
        "                        script.decompose()\n",
        "                    content = self.clean_text(body.get_text())\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting content: {str(e)}\")\n",
        "\n",
        "        return title, content\n",
        "\n",
        "    def scrape_single_article(self, url):\n",
        "        \"\"\"\n",
        "        Scrape a single article with retry logic\n",
        "\n",
        "        Args:\n",
        "            url (str): Article URL to scrape\n",
        "\n",
        "        Returns:\n",
        "            dict: Scraped article data or None if failed\n",
        "        \"\"\"\n",
        "        for attempt in range(self.retry_attempts):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=self.request_timeout)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Parse HTML\n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "                # Extract content\n",
        "                title, content = self.extract_article_content(soup)\n",
        "\n",
        "                if title and content and len(content) > 100:  # Minimum content length\n",
        "                    return {\n",
        "                        'url': url,\n",
        "                        'title': title,\n",
        "                        'content': content,\n",
        "                        'status': 'success',\n",
        "                        'content_length': len(content),\n",
        "                        'attempt': attempt + 1\n",
        "                    }\n",
        "                else:\n",
        "                    logger.warning(f\"Insufficient content for {url}\")\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                if attempt < self.retry_attempts - 1:\n",
        "                    time.sleep(1 * (attempt + 1))  # Exponential backoff\n",
        "                    continue\n",
        "                logger.error(f\"Failed to scrape {url} after {self.retry_attempts} attempts: {str(e)}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error scraping {url}: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': '',\n",
        "            'content': '',\n",
        "            'status': 'failed',\n",
        "            'content_length': 0,\n",
        "            'attempt': self.retry_attempts\n",
        "        }\n",
        "\n",
        "    def scrape_articles(self, urls):\n",
        "        \"\"\"\n",
        "        Scrape multiple articles concurrently with progress visualization\n",
        "\n",
        "        Args:\n",
        "            urls (list): List of URLs to scrape\n",
        "\n",
        "        Returns:\n",
        "            tuple: (successful_results, failed_results)\n",
        "        \"\"\"\n",
        "        print(f\"🚀 Starting to scrape {len(urls)} articles...\")\n",
        "        print(f\"⚙️  Configuration: {self.max_workers} threads, {self.request_timeout}s timeout\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress tracking\n",
        "        with tqdm(total=len(urls), desc=\"Scraping Progress\", ncols=100) as pbar:\n",
        "            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "                # Submit all tasks\n",
        "                future_to_url = {executor.submit(self.scrape_single_article, url): url for url in urls}\n",
        "\n",
        "                # Process completed tasks\n",
        "                for future in as_completed(future_to_url):\n",
        "                    result = future.result()\n",
        "\n",
        "                    if result['status'] == 'success':\n",
        "                        self.successful_scrapes.append(result)\n",
        "                        pbar.set_postfix({\n",
        "                            'Success': len(self.successful_scrapes),\n",
        "                            'Failed': len(self.failed_scrapes),\n",
        "                            'Rate': f\"{len(self.successful_scrapes)/(time.time()-start_time):.1f}/s\"\n",
        "                        })\n",
        "                    else:\n",
        "                        self.failed_scrapes.append(result)\n",
        "\n",
        "                    pbar.update(1)\n",
        "\n",
        "        end_time = time.time()\n",
        "        duration = end_time - start_time\n",
        "\n",
        "        # Print final statistics\n",
        "        print(f\"\\n📊 Scraping Results:\")\n",
        "        print(f\"✅ Successful: {len(self.successful_scrapes)}\")\n",
        "        print(f\"❌ Failed: {len(self.failed_scrapes)}\")\n",
        "        print(f\"⏱️  Total time: {duration:.2f} seconds ({duration/60:.2f} minutes)\")\n",
        "        print(f\"📈 Average rate: {len(urls)/duration:.2f} articles/second\")\n",
        "        print(f\"💾 Total content scraped: {sum(r['content_length'] for r in self.successful_scrapes):,} characters\")\n",
        "\n",
        "        return self.successful_scrapes, self.failed_scrapes\n",
        "\n",
        "def load_urls_from_text(url_text):\n",
        "    \"\"\"Load URLs from text input, handling various formats\"\"\"\n",
        "    urls = []\n",
        "    for line in url_text.strip().split('\\n'):\n",
        "        line = line.strip()\n",
        "        if line and line.startswith('http'):\n",
        "            urls.append(line)\n",
        "    return urls\n",
        "\n",
        "# MAIN EXECUTION SECTION - MODIFY THIS PART\n",
        "def main():\n",
        "    \"\"\"Main function - modify URLs here or upload file\"\"\"\n",
        "\n",
        "    print(\"🔧 MIT News Articles Scraper\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # OPTION 1: Paste URLs directly here (for testing)\n",
        "    sample_urls = [\n",
        "        \"https://news.mit.edu/2025/ianacare-builds-lifeline-for-family-caregivers-across-us-0811\",\n",
        "        \"https://news.mit.edu/2025/cloudian-helps-data-storage-keep-up-with-ai-revolution-0806\",\n",
        "        \"https://news.mit.edu/2025/ushering-new-era-suture-free-tissue-reconstruction-better-healing-0801\",\n",
        "        \"https://news.mit.edu/2025/supporting-mission-driven-space-innovation-aurelia-institute-0710\",\n",
        "        \"https://news.mit.edu/2025/new-platform-foundation-alloy-developing-advanced-metals-scale-0703\"\n",
        "    ]\n",
        "\n",
        "    # OPTION 2: Upload file method (uncomment and modify path)\n",
        "    # try:\n",
        "    #     with open('your_urls_file.txt', 'r') as f:\n",
        "    #         urls_text = f.read()\n",
        "    #         urls = load_urls_from_text(urls_text)\n",
        "    # except FileNotFoundError:\n",
        "    #     print(\"URLs file not found, using sample URLs\")\n",
        "    #     urls = sample_urls\n",
        "\n",
        "    # For now, using sample URLs\n",
        "    urls = sample_urls\n",
        "\n",
        "    if not urls:\n",
        "        print(\"❌ No URLs found! Please add URLs to the sample_urls list or upload a file.\")\n",
        "        return\n",
        "\n",
        "    print(f\"📋 Found {len(urls)} URLs to scrape\")\n",
        "\n",
        "    # Initialize scraper with optimized settings for speed\n",
        "    scraper = MITNewsScraper(\n",
        "        max_workers=20,  # Adjust based on your needs (higher = faster but more aggressive)\n",
        "        request_timeout=10,\n",
        "        retry_attempts=2\n",
        "    )\n",
        "\n",
        "    # Start scraping\n",
        "    successful_results, failed_results = scraper.scrape_articles(urls)\n",
        "\n",
        "    # Save results to CSV\n",
        "    if successful_results:\n",
        "        df = pd.DataFrame(successful_results)\n",
        "        df = df[['url', 'title', 'content', 'content_length']]  # Select relevant columns\n",
        "\n",
        "        # Save to CSV\n",
        "        output_filename = f'mit_news_articles_{int(time.time())}.csv'\n",
        "        df.to_csv(output_filename, index=False)\n",
        "        print(f\"💾 Results saved to: {output_filename}\")\n",
        "\n",
        "        # Display sample results\n",
        "        print(f\"\\n📖 Sample of scraped content:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, result in enumerate(successful_results[:2]):  # Show first 2 results\n",
        "            print(f\"Title: {result['title']}\")\n",
        "            print(f\"Content preview: {result['content'][:200]}...\")\n",
        "            print(f\"Content length: {result['content_length']} characters\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    # Save failed URLs for retry if needed\n",
        "    if failed_results:\n",
        "        failed_df = pd.DataFrame(failed_results)\n",
        "        failed_filename = f'failed_urls_{int(time.time())}.csv'\n",
        "        failed_df.to_csv(failed_filename, index=False)\n",
        "        print(f\"⚠️  Failed URLs saved to: {failed_filename}\")\n",
        "\n",
        "# Execute the scraper\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# FOR GOOGLE COLAB: Uncomment the line below to run automatically\n",
        "# main()"
      ]
    }
  ]
}