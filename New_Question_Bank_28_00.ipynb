{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwXtaD+S18ZUBk8UMqxRdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/colabcodes/blob/main/New_Question_Bank_28_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLOG11X5vGcs"
      },
      "outputs": [],
      "source": [
        "# Reading Comprehension Question Generator for Google Colab\n",
        "# Enhanced version with natural question patterns and proper formatting\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "import re\n",
        "import openai\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import files, drive\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "import psutil\n",
        "\n",
        "# Configuration and Setup\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.api_key = None\n",
        "        self.model = \"gpt-3.5-turbo\"\n",
        "        self.max_workers = 5  # Optimize for speed while avoiding rate limits\n",
        "        self.batch_size = 100  # Process in batches to manage memory\n",
        "        self.max_retries = 3\n",
        "        self.retry_delay = 2\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Get API key and file settings\n",
        "print(\"üöÄ Enhanced Reading Comprehension Question Generator\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Input configurations\n",
        "config.api_key = input(\"Enter your OpenAI API key: \").strip()\n",
        "\n",
        "# Initialize OpenAI client (works with both old and new versions)\n",
        "try:\n",
        "    # Try new OpenAI v1.0+ client\n",
        "    from openai import OpenAI\n",
        "    openai_client = OpenAI(api_key=config.api_key)\n",
        "    use_new_api = True\n",
        "    print(\"‚úÖ Using OpenAI v1.0+ API\")\n",
        "except ImportError:\n",
        "    # Fallback to old API\n",
        "    openai.api_key = config.api_key\n",
        "    openai_client = openai\n",
        "    use_new_api = False\n",
        "    print(\"‚úÖ Using OpenAI legacy API\")\n",
        "\n",
        "output_filename = input(\"Enter output filename (without extension): \").strip()\n",
        "if not output_filename:\n",
        "    output_filename = \"reading_comprehension_output\"\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"\\nüìÅ Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Enhanced question patterns with much more variety\n",
        "QUESTION_STARTERS = {\n",
        "    \"detail_comprehension\": [\n",
        "        \"Based on the passage, what\",\n",
        "        \"The text states that\",\n",
        "        \"According to the information provided\",\n",
        "        \"The passage mentions that\",\n",
        "        \"From the article, we learn that\",\n",
        "        \"The writer indicates that\",\n",
        "        \"The text reveals that\",\n",
        "        \"As described in the passage\",\n",
        "        \"The article explains that\",\n",
        "        \"The passage clearly shows that\",\n",
        "        \"We can determine from the text that\",\n",
        "        \"The information suggests that\",\n",
        "        \"The content indicates that\",\n",
        "        \"From what is written\",\n",
        "        \"The passage describes\",\n",
        "        \"The text identifies\",\n",
        "        \"According to the details given\",\n",
        "        \"The article specifies that\",\n",
        "        \"The writing confirms that\",\n",
        "        \"Based on the facts presented\"\n",
        "    ],\n",
        "    \"inference_reasoning\": [\n",
        "        \"It can be concluded that\",\n",
        "        \"The passage suggests that\",\n",
        "        \"We can infer that\",\n",
        "        \"The text implies that\",\n",
        "        \"One can reasonably assume that\",\n",
        "        \"The evidence points to\",\n",
        "        \"The information leads us to believe that\",\n",
        "        \"Based on the context\",\n",
        "        \"The underlying message is that\",\n",
        "        \"The passage hints that\",\n",
        "        \"We might deduce that\",\n",
        "        \"The text indicates that\",\n",
        "        \"From the evidence presented\",\n",
        "        \"The overall tone suggests that\",\n",
        "        \"Reading between the lines\",\n",
        "        \"The subtext reveals that\",\n",
        "        \"The implications are that\",\n",
        "        \"The logical conclusion is that\",\n",
        "        \"The passage conveys that\",\n",
        "        \"The deeper meaning suggests\"\n",
        "    ],\n",
        "    \"opinion_attitude\": [\n",
        "        \"The author's perspective on\",\n",
        "        \"The writer's viewpoint regarding\",\n",
        "        \"The author feels that\",\n",
        "        \"The writer's stance on\",\n",
        "        \"The author's opinion about\",\n",
        "        \"The text reflects a\",\n",
        "        \"The author demonstrates\",\n",
        "        \"The writer shows\",\n",
        "        \"The author's attitude toward\",\n",
        "        \"The writer expresses\",\n",
        "        \"The author believes that\",\n",
        "        \"The text conveys the author's\",\n",
        "        \"The writer's position is that\",\n",
        "        \"The author's approach to\",\n",
        "        \"The writer regards\",\n",
        "        \"The author considers\",\n",
        "        \"The text shows the author\",\n",
        "        \"The writer's judgment about\",\n",
        "        \"The author's take on\",\n",
        "        \"The writer's assessment of\"\n",
        "    ],\n",
        "    \"main_idea\": [\n",
        "        \"The central theme of this passage is\",\n",
        "        \"This text primarily focuses on\",\n",
        "        \"The main point of the passage is\",\n",
        "        \"The passage is essentially about\",\n",
        "        \"The primary subject of this text is\",\n",
        "        \"The core message of the passage is\",\n",
        "        \"This article mainly discusses\",\n",
        "        \"The fundamental topic is\",\n",
        "        \"The passage centers on\",\n",
        "        \"The key focus of the text is\",\n",
        "        \"The predominant theme is\",\n",
        "        \"The passage's main concern is\",\n",
        "        \"The central focus involves\",\n",
        "        \"The primary emphasis is on\",\n",
        "        \"The text is fundamentally about\",\n",
        "        \"The overarching theme is\",\n",
        "        \"The passage mainly addresses\",\n",
        "        \"The principal subject matter is\",\n",
        "        \"The text chiefly examines\",\n",
        "        \"The main thrust of the passage is\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Analysis cleaning patterns - phrases to remove from the start of analysis\n",
        "ANALYSIS_PHRASES_TO_REMOVE = [\n",
        "    r'^This option is correct as the passage explains that\\s*',\n",
        "    r'^This is correct because\\s*',\n",
        "    r'^The passage supports this by\\s*',\n",
        "    r'^The text clearly indicates that\\s*',\n",
        "    r'^Based on the information provided,?\\s*',\n",
        "    r'^The author explicitly mentions that\\s*',\n",
        "    r'^The passage demonstrates that\\s*',\n",
        "    r'^The evidence shows that\\s*',\n",
        "    r'^The text confirms that\\s*',\n",
        "    r'^The article establishes that\\s*',\n",
        "    r'^The writing reveals that\\s*',\n",
        "    r'^The content supports this through\\s*',\n",
        "    r'^The passage provides evidence that\\s*',\n",
        "    r'^The information confirms that\\s*',\n",
        "    r'^The text validates this by\\s*',\n",
        "    r'^The author\\'s words support this because\\s*',\n",
        "    r'^The passage substantiates this through\\s*',\n",
        "    r'^The evidence points to this since\\s*',\n",
        "    r'^The text backs this up by\\s*',\n",
        "    r'^The information corroborates this because\\s*',\n",
        "    r'^The passage verifies this through\\s*',\n",
        "    r'^The answer is [A-D] because\\s*',\n",
        "    r'^[A-D] is correct because\\s*',\n",
        "    r'^This option is correct because\\s*',\n",
        "    r'^This answer is right because\\s*',\n",
        "    r'^The correct answer is [A-D] because\\s*',\n",
        "    r'^Option [A-D] is correct as\\s*',\n",
        "    r'^This choice is right because\\s*'\n",
        "]\n",
        "\n",
        "# Domain classification function\n",
        "def classify_domain(text):\n",
        "    \"\"\"Classify article domain based on content keywords\"\"\"\n",
        "    domain_keywords = {\n",
        "        \"Technology\": [\"technology\", \"tech\", \"digital\", \"AI\", \"software\", \"internet\", \"computer\", \"smartphone\", \"app\"],\n",
        "        \"Science\": [\"research\", \"study\", \"scientist\", \"experiment\", \"discovery\", \"medical\", \"health\", \"biology\"],\n",
        "        \"Sports\": [\"sport\", \"athlete\", \"game\", \"competition\", \"team\", \"player\", \"championship\", \"olympic\"],\n",
        "        \"Business\": [\"business\", \"company\", \"market\", \"economy\", \"finance\", \"investment\", \"corporate\", \"industry\"],\n",
        "        \"Politics\": [\"government\", \"political\", \"policy\", \"election\", \"politician\", \"law\", \"congress\", \"president\"],\n",
        "        \"Entertainment\": [\"movie\", \"music\", \"celebrity\", \"film\", \"actor\", \"entertainment\", \"show\", \"television\"],\n",
        "        \"Education\": [\"school\", \"student\", \"education\", \"teacher\", \"university\", \"learning\", \"academic\", \"classroom\"],\n",
        "        \"Environment\": [\"environment\", \"climate\", \"nature\", \"pollution\", \"sustainable\", \"green\", \"ecology\", \"conservation\"],\n",
        "        \"History\": [\"history\", \"historical\", \"ancient\", \"past\", \"century\", \"era\", \"civilization\", \"culture\"],\n",
        "        \"Literature\": [\"book\", \"author\", \"novel\", \"poetry\", \"literature\", \"writing\", \"story\", \"literary\"]\n",
        "    }\n",
        "\n",
        "    text_lower = text.lower()\n",
        "    domain_scores = {}\n",
        "\n",
        "    for domain, keywords in domain_keywords.items():\n",
        "        score = sum(text_lower.count(keyword) for keyword in keywords)\n",
        "        if score > 0:\n",
        "            domain_scores[domain] = score\n",
        "\n",
        "    return max(domain_scores, key=domain_scores.get) if domain_scores else \"General\"\n",
        "\n",
        "# Stage classification based on text complexity\n",
        "def classify_stage(text):\n",
        "    \"\"\"Classify educational stage based on text complexity\"\"\"\n",
        "    word_count = len(text.split())\n",
        "    avg_sentence_length = word_count / max(text.count('.') + text.count('!') + text.count('?'), 1)\n",
        "\n",
        "    # Simple heuristic based on length and complexity\n",
        "    if word_count < 200 or avg_sentence_length < 15:\n",
        "        return \"Middle School\"\n",
        "    else:\n",
        "        return \"High School\"\n",
        "\n",
        "# Clean and anonymize text while preserving formatting\n",
        "def clean_and_anonymize_text(text):\n",
        "    \"\"\"Clean text and anonymize personal information while preserving line breaks\"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    text = str(text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Anonymize patterns\n",
        "    anonymize_patterns = [\n",
        "        (r'\\b\\d{3}-\\d{3}-\\d{4}\\b', 'xxx-xxx-xxxx'),  # Phone numbers\n",
        "        (r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', 'xxxx@xxxx.xxx'),  # Email\n",
        "        (r'\\b\\d{3,4}\\s?\\d{3,4}\\s?\\d{4}\\b', 'xxxxxxxxxxxx'),  # Various phone formats\n",
        "        (r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{4}\\b', 'xxxx-xxxx-xxxx-xxxx'),  # Credit card format\n",
        "    ]\n",
        "\n",
        "    for pattern, replacement in anonymize_patterns:\n",
        "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
        "\n",
        "    # PRESERVE original line breaks and spacing - only clean excessive whitespace\n",
        "    # Remove only excessive line breaks (4 or more in a row)\n",
        "    text = re.sub(r'\\n{4,}', '\\n\\n\\n', text)\n",
        "    # Remove excessive spaces within lines (3 or more spaces become 2)\n",
        "    text = re.sub(r'[ \\t]{3,}', '  ', text)\n",
        "    # Clean up trailing/leading whitespace on each line while preserving line breaks\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = [line.rstrip() for line in lines]  # Remove trailing spaces from each line\n",
        "    text = '\\n'.join(cleaned_lines)\n",
        "\n",
        "    # Only strip the very beginning and end\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Generate hash ID\n",
        "def generate_hash_id(text):\n",
        "    \"\"\"Generate unique hash ID for the article\"\"\"\n",
        "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "# Enhanced answer extraction and formatting\n",
        "def extract_clean_answer(answer_text):\n",
        "    \"\"\"Extract just the letter from answer field, removing explanations\"\"\"\n",
        "    if not answer_text:\n",
        "        return \"\"\n",
        "\n",
        "    # Clean the answer text\n",
        "    answer_text = str(answer_text).strip()\n",
        "\n",
        "    # Look for single letter answers (A, B, C, D) at the beginning\n",
        "    match = re.match(r'^([A-D])\\b', answer_text.upper())\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "\n",
        "    # If no clear match, try to find any A, B, C, or D\n",
        "    letters = re.findall(r'\\b([A-D])\\b', answer_text.upper())\n",
        "    if letters:\n",
        "        return letters[0]\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# Enhanced options formatting\n",
        "def format_options_with_labels(options_list):\n",
        "    \"\"\"Format options with proper A, B, C, D labels\"\"\"\n",
        "    if not options_list or not isinstance(options_list, list):\n",
        "        return []\n",
        "\n",
        "    formatted_options = []\n",
        "    labels = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    for i, option in enumerate(options_list[:4]):  # Max 4 options\n",
        "        if i < len(labels):\n",
        "            # Clean the option text and add proper label\n",
        "            clean_option = str(option).strip()\n",
        "            # Remove any existing A), B), etc. labels\n",
        "            clean_option = re.sub(r'^[A-D][).]\\s*', '', clean_option)\n",
        "            formatted_options.append(f\"{labels[i]}. {clean_option}\")\n",
        "\n",
        "    return formatted_options\n",
        "\n",
        "# OpenAI API call with retry logic\n",
        "def call_openai_with_retry(messages, max_retries=3):\n",
        "    \"\"\"Call OpenAI API with retry logic for rate limiting\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if use_new_api:\n",
        "                # New OpenAI v1.0+ API\n",
        "                response = openai_client.chat.completions.create(\n",
        "                    model=config.model,\n",
        "                    messages=messages,\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.8  # Increased for more variety\n",
        "                )\n",
        "                return response.choices[0].message.content.strip()\n",
        "            else:\n",
        "                # Legacy OpenAI API\n",
        "                response = openai_client.ChatCompletion.create(\n",
        "                    model=config.model,\n",
        "                    messages=messages,\n",
        "                    max_tokens=2000,\n",
        "                    temperature=0.8  # Increased for more variety\n",
        "                )\n",
        "                return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            error_str = str(e).lower()\n",
        "            if \"rate\" in error_str or \"limit\" in error_str or \"429\" in error_str:\n",
        "                if attempt < max_retries - 1:\n",
        "                    wait_time = (2 ** attempt) * config.retry_delay\n",
        "                    print(f\"Rate limit hit, waiting {wait_time} seconds...\")\n",
        "                    time.sleep(wait_time)\n",
        "                else:\n",
        "                    raise\n",
        "            elif attempt < max_retries - 1:\n",
        "                print(f\"Error occurred: {str(e)}, retrying...\")\n",
        "                time.sleep(config.retry_delay)\n",
        "            else:\n",
        "                raise\n",
        "    return None\n",
        "\n",
        "# Generate questions for a single article\n",
        "def generate_questions_for_article(article_data, article_index):\n",
        "    \"\"\"Generate 5-7 MC questions for a single article with natural variety\"\"\"\n",
        "    try:\n",
        "        article_text, source_url = article_data\n",
        "\n",
        "        # Clean and prepare text\n",
        "        clean_text = clean_and_anonymize_text(article_text)\n",
        "        if not clean_text or len(clean_text.split()) < 50:\n",
        "            return None\n",
        "\n",
        "        # Classify domain and stage\n",
        "        domain = classify_domain(clean_text)\n",
        "        stage = classify_stage(clean_text)\n",
        "\n",
        "        # Randomly select question starters for variety\n",
        "        detail_starters = random.sample(QUESTION_STARTERS[\"detail_comprehension\"], 2)\n",
        "        inference_starters = random.sample(QUESTION_STARTERS[\"inference_reasoning\"], 2)\n",
        "        opinion_starters = random.sample(QUESTION_STARTERS[\"opinion_attitude\"], 2)\n",
        "        main_idea_starter = random.choice(QUESTION_STARTERS[\"main_idea\"])\n",
        "\n",
        "        # Create enhanced prompt with natural variety\n",
        "        prompt = f\"\"\"\n",
        "You are a professional educator creating reading comprehension questions. Create EXACTLY 7 multiple choice questions based on this article. Make each question sound natural and unique - avoid repetitive patterns.\n",
        "\n",
        "ARTICLE:\n",
        "{clean_text}\n",
        "\n",
        "Create questions using these varied approaches:\n",
        "\n",
        "DETAIL COMPREHENSION (2 questions) - Use these starters:\n",
        "1. \"{detail_starters[0]}...\"\n",
        "2. \"{detail_starters[1]}...\"\n",
        "\n",
        "INFERENCE & REASONING (2 questions) - Use these starters:\n",
        "3. \"{inference_starters[0]}...\"\n",
        "4. \"{inference_starters[1]}...\"\n",
        "\n",
        "OPINION & ATTITUDE (2 questions) - Use these starters:\n",
        "5. \"{opinion_starters[0]}...\"\n",
        "6. \"{opinion_starters[1]}...\"\n",
        "\n",
        "MAIN IDEA (1 question) - Use this starter:\n",
        "7. \"{main_idea_starter}...\"\n",
        "\n",
        "CRITICAL REQUIREMENTS:\n",
        "- Questions must sound natural and conversational\n",
        "- Base ALL questions on specific content from this article\n",
        "- Provide 4 options for each question (will be labeled A, B, C, D automatically)\n",
        "- Answer field should contain ONLY the letter (A, B, C, or D)\n",
        "- Make wrong answers plausible but clearly incorrect\n",
        "- Vary your language and avoid repetitive phrases\n",
        "- Analysis should be ONLY the direct explanation - NO introductory phrases like \"This is correct because\" or \"The passage shows that\" - just give the pure explanation\n",
        "\n",
        "Format as JSON:\n",
        "[\n",
        "  {{\n",
        "    \"type\": \"detail_comprehension\",\n",
        "    \"question\": \"Complete question using the starter provided...\",\n",
        "    \"options\": [\"First option\", \"Second option\", \"Third option\", \"Fourth option\"],\n",
        "    \"answer\": \"B\",\n",
        "    \"analysis\": \"The company expanded operations to five new countries last year according to the financial report.\"\n",
        "  }}\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert educator creating diverse, natural-sounding reading comprehension questions. Avoid repetitive patterns and make each question unique while maintaining educational value.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = call_openai_with_retry(messages)\n",
        "\n",
        "        if not response:\n",
        "            return None\n",
        "\n",
        "        # Parse JSON response\n",
        "        try:\n",
        "            questions_data = json.loads(response)\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback: try to extract JSON from response\n",
        "            json_match = re.search(r'\\[.*\\]', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                try:\n",
        "                    questions_data = json.loads(json_match.group())\n",
        "                except:\n",
        "                    return None\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # Process questions into required format\n",
        "        processed_questions = []\n",
        "        question_types = [\"detail_comprehension\", \"detail_comprehension\", \"inference_reasoning\",\n",
        "                         \"inference_reasoning\", \"opinion_attitude\", \"opinion_attitude\", \"main_idea\"]\n",
        "\n",
        "        for i, q_data in enumerate(questions_data[:7]):  # Max 7 questions\n",
        "            try:\n",
        "                hash_id = generate_hash_id(f\"{clean_text}_{i}\")\n",
        "                word_count = len(clean_text.split())\n",
        "\n",
        "                # Clean and format the answer\n",
        "                raw_answer = q_data.get('answer', '')\n",
        "                clean_answer = extract_clean_answer(raw_answer)\n",
        "\n",
        "                # Format options with A, B, C, D labels\n",
        "                raw_options = q_data.get('options', [])\n",
        "                formatted_options = format_options_with_labels(raw_options)\n",
        "\n",
        "                # Clean analysis - remove ALL introductory phrases, keep only pure explanation\n",
        "                raw_analysis = q_data.get('analysis', '')\n",
        "                clean_analysis = raw_analysis\n",
        "\n",
        "                # Remove all common analysis starter phrases\n",
        "                for pattern in ANALYSIS_PHRASES_TO_REMOVE:\n",
        "                    clean_analysis = re.sub(pattern, '', clean_analysis, flags=re.IGNORECASE)\n",
        "\n",
        "                # Clean up any remaining common patterns\n",
        "                clean_analysis = re.sub(r'^(The|This|It)\\s+(is\\s+)?(correct|right|true)\\s+(because|as|since)\\s*', '', clean_analysis, flags=re.IGNORECASE)\n",
        "                clean_analysis = re.sub(r'^(According to|Based on|From)\\s+the\\s+(passage|text|article),?\\s*', '', clean_analysis, flags=re.IGNORECASE)\n",
        "\n",
        "                # Clean up the result\n",
        "                clean_analysis = clean_analysis.strip()\n",
        "\n",
        "                # Ensure it starts with a capital letter if there's content\n",
        "                if clean_analysis and not clean_analysis[0].isupper():\n",
        "                    clean_analysis = clean_analysis[0].upper() + clean_analysis[1:]\n",
        "\n",
        "                # If analysis is empty after cleaning, provide a minimal fallback\n",
        "                if not clean_analysis.strip():\n",
        "                    clean_analysis = \"The passage provides specific information supporting this option.\"\n",
        "\n",
        "                question_obj = {\n",
        "                    \"id\": hash_id,\n",
        "                    \"text\": clean_text,  # Original article text\n",
        "                    \"meta\": {\n",
        "                        \"data_info\": {\n",
        "                            \"lang\": \"en\",\n",
        "                            \"source\": source_url,\n",
        "                            \"type\": \"Reading Comprehension\",\n",
        "                            \"processing_date\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
        "                            \"Question\": q_data.get('question', ''),\n",
        "                            \"Options\": formatted_options,  # Properly formatted with A, B, C, D\n",
        "                            \"Answer\": clean_answer,  # Just the letter\n",
        "                            \"Analysis\": clean_analysis,  # Natural analysis without repetitive starters\n",
        "                            \"Question_Type\": question_types[i] if i < len(question_types) else q_data.get('type', ''),\n",
        "                            \"word_count\": word_count\n",
        "                        },\n",
        "                        \"subject_info\": {\n",
        "                            \"subject\": \"English\",\n",
        "                            \"domain\": domain,\n",
        "                            \"stage\": stage\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "                processed_questions.append(question_obj)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing question {i} for article {article_index}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return processed_questions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {article_index}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Main processing function\n",
        "def process_articles_batch(articles_batch, start_index):\n",
        "    \"\"\"Process a batch of articles\"\"\"\n",
        "    results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=config.max_workers) as executor:\n",
        "        # Submit tasks\n",
        "        future_to_index = {\n",
        "            executor.submit(generate_questions_for_article, article_data, start_index + i): start_index + i\n",
        "            for i, article_data in enumerate(articles_batch) if article_data and article_data[0] and not pd.isna(article_data[0])\n",
        "        }\n",
        "\n",
        "        # Collect results\n",
        "        for future in as_completed(future_to_index):\n",
        "            index = future_to_index[future]\n",
        "            try:\n",
        "                questions = future.result(timeout=60)  # 60 second timeout\n",
        "                if questions:\n",
        "                    results.extend(questions)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing article {index}: {str(e)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Load and process CSV file\n",
        "print(\"\\nüìÑ Upload your CSV file with articles...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "print(f\"Processing: {csv_filename}\")\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(csv_filename)\n",
        "print(f\"Loaded {len(df)} articles\")\n",
        "\n",
        "# Detect columns (assume first column is articles, second is source URLs)\n",
        "article_column = df.columns[0]\n",
        "source_url_column = df.columns[1] if len(df.columns) > 1 else None\n",
        "\n",
        "# Create paired data (article, source_url)\n",
        "articles_data = []\n",
        "for idx, row in df.iterrows():\n",
        "    if pd.notna(row[article_column]):\n",
        "        source_url = row[source_url_column] if source_url_column and pd.notna(row[source_url_column]) else \"Exam Question Bank\"\n",
        "        articles_data.append((row[article_column], source_url))\n",
        "\n",
        "print(f\"Processing {len(articles_data)} valid articles with source URLs...\")\n",
        "print(\"Enhanced version will create more natural, varied questions. This may take several hours for 1M articles.\")\n",
        "\n",
        "# Process articles in batches\n",
        "all_results = []\n",
        "batch_size = config.batch_size\n",
        "\n",
        "# Progress tracking\n",
        "total_batches = (len(articles_data) + batch_size - 1) // batch_size\n",
        "processed_articles = 0\n",
        "\n",
        "print(f\"\\nüîÑ Starting enhanced processing in {total_batches} batches...\")\n",
        "\n",
        "for batch_num in tqdm(range(0, len(articles_data), batch_size), desc=\"Processing batches\"):\n",
        "    batch = articles_data[batch_num:batch_num + batch_size]\n",
        "    start_time = time.time()\n",
        "\n",
        "    batch_results = process_articles_batch(batch, batch_num)\n",
        "    all_results.extend(batch_results)\n",
        "\n",
        "    processed_articles += len(batch)\n",
        "    batch_time = time.time() - start_time\n",
        "\n",
        "    # Memory management\n",
        "    if batch_num % 10 == 0:  # Every 10 batches\n",
        "        gc.collect()\n",
        "\n",
        "    # Progress info\n",
        "    questions_in_batch = len(batch_results)\n",
        "    avg_time_per_article = batch_time / len(batch) if batch else 0\n",
        "\n",
        "    print(f\"Batch {batch_num//batch_size + 1}/{total_batches}: {questions_in_batch} questions generated in {batch_time:.1f}s (avg: {avg_time_per_article:.2f}s/article)\")\n",
        "\n",
        "    # Save intermediate results every 1000 articles\n",
        "    if processed_articles % 1000 == 0:\n",
        "        temp_filename = f\"/content/drive/MyDrive/{output_filename}_temp_{processed_articles}.json\"\n",
        "        with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "        print(f\"üíæ Saved intermediate results: {len(all_results)} questions\")\n",
        "\n",
        "print(f\"\\n‚úÖ Enhanced processing complete! Generated {len(all_results)} questions from {processed_articles} articles\")\n",
        "\n",
        "# Save final results\n",
        "if all_results:\n",
        "    # Save JSON\n",
        "    json_filename = f\"/content/drive/MyDrive/{output_filename}.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Create CSV version with enhanced structure and preserved formatting\n",
        "    csv_data = []\n",
        "    for item in all_results:\n",
        "        # Format options as a clean string\n",
        "        options_str = \" | \".join(item['meta']['data_info']['Options']) if item['meta']['data_info']['Options'] else \"\"\n",
        "\n",
        "        # Preserve original text formatting in CSV\n",
        "        preserved_text = item['text'].replace('\\n', '\\\\n')  # Escape newlines for CSV\n",
        "\n",
        "        flat_item = {\n",
        "            'id': item['id'],\n",
        "            'text': preserved_text,  # Preserved formatting with escaped newlines\n",
        "            'lang': item['meta']['data_info']['lang'],\n",
        "            'source': item['meta']['data_info']['source'],\n",
        "            'type': item['meta']['data_info']['type'],\n",
        "            'processing_date': item['meta']['data_info']['processing_date'],\n",
        "            'question': item['meta']['data_info']['Question'],\n",
        "            'options': options_str,  # Clean formatted options\n",
        "            'answer': item['meta']['data_info']['Answer'],  # Just the letter\n",
        "            'analysis': item['meta']['data_info']['Analysis'],  # Natural analysis\n",
        "            'question_type': item['meta']['data_info'].get('Question_Type', ''),\n",
        "            'word_count': item['meta']['data_info'].get('word_count', 0),\n",
        "            'subject': item['meta']['subject_info']['subject'],\n",
        "            'domain': item['meta']['subject_info']['domain'],\n",
        "            'stage': item['meta']['subject_info']['stage']\n",
        "        }\n",
        "        csv_data.append(flat_item)\n",
        "\n",
        "    csv_df = pd.DataFrame(csv_data)\n",
        "    csv_filename = f\"/content/drive/MyDrive/{output_filename}.csv\"\n",
        "    # Use quoting to preserve formatting\n",
        "    csv_df.to_csv(csv_filename, index=False, encoding='utf-8', quoting=1)  # QUOTE_ALL\n",
        "\n",
        "    print(f\"\\nüéâ Enhanced results saved successfully!\")\n",
        "    print(f\"üìÅ JSON file: {json_filename}\")\n",
        "    print(f\"üìÅ CSV file: {csv_filename}\")\n",
        "    print(f\"üìä Total questions generated: {len(all_results)}\")\n",
        "\n",
        "    # Enhanced summary statistics\n",
        "    domains = csv_df['domain'].value_counts()\n",
        "    stages = csv_df['stage'].value_counts()\n",
        "    question_types = csv_df['question_type'].value_counts()\n",
        "\n",
        "    # Check answer distribution\n",
        "    answer_dist = csv_df['answer'].value_counts()\n",
        "\n",
        "    print(f\"\\nüìà Enhanced Summary Statistics:\")\n",
        "    print(f\"Domains: {dict(domains)}\")\n",
        "    print(f\"Stages: {dict(stages)}\")\n",
        "    print(f\"Question Types: {dict(question_types)}\")\n",
        "    print(f\"Answer Distribution: {dict(answer_dist)}\")\n",
        "\n",
        "    # Quality checks\n",
        "    clean_answers = csv_df['answer'].str.match(r'^[A-D]$').sum()\n",
        "    print(f\"‚úÖ Clean answers (A-D only): {clean_answers}/{len(csv_df)} ({clean_answers/len(csv_df)*100:.1f}%)\")\n",
        "\n",
        "    proper_options = csv_df['options'].str.contains(r'A\\.|B\\.|C\\.|D\\.').sum()\n",
        "    print(f\"‚úÖ Properly formatted options: {proper_options}/{len(csv_df)} ({proper_options/len(csv_df)*100:.1f}%)\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No questions were generated. Please check your input data and API key.\")\n",
        "\n",
        "print(\"\\nüèÅ Enhanced process completed with improved quality!\")"
      ]
    }
  ]
}