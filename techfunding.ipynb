{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbY2tZz4+wW+NgkW0nZ1Vf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/colabcodes/blob/main/techfunding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iixclH-vbvS4"
      },
      "outputs": [],
      "source": [
        "# ==============================================\n",
        "# ðŸš€ One-Click Tech Funding News Scraper (Colab)\n",
        "# ==============================================\n",
        "\n",
        "# Install dependencies\n",
        "!pip install requests beautifulsoup4 tqdm\n",
        "\n",
        "# Imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import threading\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# --------------------------\n",
        "# SCRAPER CLASS\n",
        "# --------------------------\n",
        "class TechNewsContentScraper:\n",
        "    def __init__(self, max_workers=20, delay_between_requests=0.1):\n",
        "        self.max_workers = max_workers\n",
        "        self.delay_between_requests = delay_between_requests\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
        "        })\n",
        "        self.results = []\n",
        "        self.failed_urls = []\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        if not text: return \"\"\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def extract_article_content(self, soup, url):\n",
        "        try:\n",
        "            title = \"\"\n",
        "            for selector in ['h1', '.entry-title', '.post-title']:\n",
        "                elem = soup.select_one(selector)\n",
        "                if elem:\n",
        "                    title = self.clean_text(elem.get_text())\n",
        "                    break\n",
        "\n",
        "            content = \"\"\n",
        "            main = None\n",
        "            for selector in ['.entry-content','.post-content','article']:\n",
        "                main = soup.select_one(selector)\n",
        "                if main: break\n",
        "\n",
        "            if main:\n",
        "                for bad in main.select('script,style,aside,nav,.share,.ads'):\n",
        "                    bad.decompose()\n",
        "                parts = [self.clean_text(e.get_text()) for e in main.find_all(['p','h2','h3']) if len(e.get_text())>20]\n",
        "                content = \" \".join(parts)\n",
        "\n",
        "            return title, content\n",
        "        except:\n",
        "            return \"\", \"\"\n",
        "\n",
        "    def scrape_single_url(self, url):\n",
        "        try:\n",
        "            time.sleep(self.delay_between_requests)\n",
        "            r = self.session.get(url, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            soup = BeautifulSoup(r.content, 'html.parser')\n",
        "            title, content = self.extract_article_content(soup, url)\n",
        "            return {'url':url,'title':title,'content':content,'status':'success','content_length':len(content)}\n",
        "        except Exception as e:\n",
        "            return {'url':url,'title':'','content':'','status':str(e),'content_length':0}\n",
        "\n",
        "    def scrape_all_urls(self, urls):\n",
        "        results = []\n",
        "        with ThreadPoolExecutor(max_workers=self.max_workers) as ex:\n",
        "            futures = {ex.submit(self.scrape_single_url,u):u for u in urls}\n",
        "            with tqdm(total=len(urls),desc=\"Scraping\") as bar:\n",
        "                for f in as_completed(futures):\n",
        "                    results.append(f.result())\n",
        "                    bar.update(1)\n",
        "        return results\n",
        "\n",
        "# --------------------------\n",
        "# MAIN RUN FUNCTION\n",
        "# --------------------------\n",
        "def run_complete_scraper(urls_file_path, output_file='scraped_articles.csv', max_workers=20):\n",
        "    scraper = TechNewsContentScraper(max_workers=max_workers, delay_between_requests=0.05)\n",
        "    with open(urls_file_path,'r') as f:\n",
        "        urls = [u.strip() for u in f if u.strip()]\n",
        "    print(f\"ðŸ“‚ Loaded {len(urls)} URLs\")\n",
        "    results = scraper.scrape_all_urls(urls)\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_file,index=False,encoding='utf-8')\n",
        "    print(f\"ðŸ’¾ Saved {len(df)} records to {output_file}\")\n",
        "    return df\n",
        "\n",
        "# --------------------------\n",
        "# ðŸš€ AUTO EXECUTION\n",
        "# --------------------------\n",
        "# Upload URL file (urls.txt) automatically\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]  # first uploaded file\n",
        "\n",
        "# Run scraper\n",
        "df = run_complete_scraper(filename, 'scraped_articles.csv', max_workers=20)\n",
        "\n",
        "# Show preview\n",
        "df.head()\n",
        "\n",
        "# --------------------------\n",
        "# ðŸ“Š Visualization\n",
        "# --------------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Success vs Failed\n",
        "df['success'] = df['status'] == 'success'\n",
        "counts = df['success'].value_counts()\n",
        "plt.pie(counts, labels=['Success','Failed'], autopct='%1.1f%%')\n",
        "plt.title(\"Scraping Success Rate\")\n",
        "plt.show()\n",
        "\n",
        "# Content length distribution\n",
        "df[df['success']]['content_length'].hist(bins=40, figsize=(8,4))\n",
        "plt.title(\"Article Length Distribution\")\n",
        "plt.xlabel(\"Characters\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ]
    }
  ]
}